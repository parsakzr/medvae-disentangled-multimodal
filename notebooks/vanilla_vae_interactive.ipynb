{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566368e1",
   "metadata": {},
   "source": [
    "# Vanilla VAE Interactive Notebook ðŸ§ ðŸ“Š\n",
    "\n",
    "This notebook provides an interactive interface for exploring a vanilla Variational Autoencoder (VAE) trained on medical imaging data.\n",
    "\n",
    "## Features:\n",
    "- ðŸ”„ **Reconstruct samples** from the dataset\n",
    "- ðŸŽ¨ **Generate new images** from the latent space  \n",
    "- ðŸ“Š **Interactive visualizations** with widgets\n",
    "- ðŸ” **Compare** original vs reconstructed images\n",
    "- ðŸŽ¯ **Single modality** focus for detailed analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17608ec",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90500ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Notebook directory: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal/notebooks\n",
      "ðŸ“ Project root: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal\n",
      "âœ… All libraries imported successfully!\n",
      "ðŸ”§ PyTorch version: 2.7.1\n",
      "ðŸŽ¯ Device available: CPU\n",
      "âœ… All libraries imported successfully!\n",
      "ðŸ”§ PyTorch version: 2.7.1\n",
      "ðŸŽ¯ Device available: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the project root to the Python path\n",
    "# Get the notebook directory and go up one level to the project root\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent  # Go up one level from notebooks/ to project root\n",
    "print(f\"ðŸ“ Notebook directory: {notebook_dir}\")\n",
    "print(f\"ðŸ“ Project root: {project_root}\")\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"âœ… Added project root to Python path\")\n",
    "\n",
    "# Import project modules\n",
    "from src.models import BaseVAE, BetaVAE\n",
    "from src.data import MedMNISTDataModule  # Corrected import path\n",
    "from src.utils import compute_reconstruction_metrics\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ”§ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸŽ¯ Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3a03f",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb4a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸  Using device: cpu\n",
      "âš™ï¸ Configuration loaded successfully!\n",
      "ðŸ“ Checkpoints directory: logs/checkpoints\n",
      "ðŸ“Š Available datasets: ['chestmnist', 'pneumoniamnist']\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸  Using device: {device}\")\n",
    "\n",
    "# Model configuration for vanilla VAE\n",
    "MODEL_CONFIG = {\n",
    "    \"input_channels\": 1,      # Grayscale images (ChestMNIST)\n",
    "    \"latent_dim\": 128,        # Latent space dimension\n",
    "    \"hidden_channels\": 128,   # Hidden layer channels\n",
    "    \"resolution\": 28,         # Image resolution\n",
    "    \"ch_mult\": (1, 2, 4, 8),  # Channel multipliers\n",
    "    \"num_res_blocks\": 2,      # Number of residual blocks\n",
    "    \"attn_resolutions\": [16], # Attention at specific resolutions\n",
    "}\n",
    "\n",
    "# Paths (update these based on your trained models)\n",
    "CHECKPOINTS_DIR = Path(\"logs/checkpoints\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Available datasets for vanilla VAE (single modality)\n",
    "DATASETS = {\n",
    "    \"chestmnist\": {\n",
    "        \"name\": \"ChestMNIST\", \n",
    "        \"channels\": 1, \n",
    "        \"description\": \"Chest X-Ray Images\"\n",
    "    },\n",
    "    \"pneumoniamnist\": {\n",
    "        \"name\": \"PneumoniaMNIST\", \n",
    "        \"channels\": 1, \n",
    "        \"description\": \"Pneumonia X-Ray Images\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Configuration loaded successfully!\")\n",
    "print(f\"ðŸ“ Checkpoints directory: {CHECKPOINTS_DIR}\")\n",
    "print(f\"ðŸ“Š Available datasets: {list(DATASETS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438bc01",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4625595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading checkpoint from: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal/logs/checkpoints/chest_base_vae_quick-epoch=04-val/loss=0.040.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BaseVAE:\n\tMissing key(s) in state_dict: \"encoder.down.0.block.1.norm1.weight\", \"encoder.down.0.block.1.norm1.bias\", \"encoder.down.0.block.1.conv1.weight\", \"encoder.down.0.block.1.conv1.bias\", \"encoder.down.0.block.1.norm2.weight\", \"encoder.down.0.block.1.norm2.bias\", \"encoder.down.0.block.1.conv2.weight\", \"encoder.down.0.block.1.conv2.bias\", \"encoder.down.1.block.1.norm1.weight\", \"encoder.down.1.block.1.norm1.bias\", \"encoder.down.1.block.1.conv1.weight\", \"encoder.down.1.block.1.conv1.bias\", \"encoder.down.1.block.1.norm2.weight\", \"encoder.down.1.block.1.norm2.bias\", \"encoder.down.1.block.1.conv2.weight\", \"encoder.down.1.block.1.conv2.bias\", \"encoder.down.2.block.1.norm1.weight\", \"encoder.down.2.block.1.norm1.bias\", \"encoder.down.2.block.1.conv1.weight\", \"encoder.down.2.block.1.conv1.bias\", \"encoder.down.2.block.1.norm2.weight\", \"encoder.down.2.block.1.norm2.bias\", \"encoder.down.2.block.1.conv2.weight\", \"encoder.down.2.block.1.conv2.bias\", \"encoder.down.2.downsample.conv.weight\", \"encoder.down.2.downsample.conv.bias\", \"encoder.down.3.block.0.norm1.weight\", \"encoder.down.3.block.0.norm1.bias\", \"encoder.down.3.block.0.conv1.weight\", \"encoder.down.3.block.0.conv1.bias\", \"encoder.down.3.block.0.norm2.weight\", \"encoder.down.3.block.0.norm2.bias\", \"encoder.down.3.block.0.conv2.weight\", \"encoder.down.3.block.0.conv2.bias\", \"encoder.down.3.block.0.nin_shortcut.weight\", \"encoder.down.3.block.0.nin_shortcut.bias\", \"encoder.down.3.block.1.norm1.weight\", \"encoder.down.3.block.1.norm1.bias\", \"encoder.down.3.block.1.conv1.weight\", \"encoder.down.3.block.1.conv1.bias\", \"encoder.down.3.block.1.norm2.weight\", \"encoder.down.3.block.1.norm2.bias\", \"encoder.down.3.block.1.conv2.weight\", \"encoder.down.3.block.1.conv2.bias\", \"decoder.up.0.block.2.norm1.weight\", \"decoder.up.0.block.2.norm1.bias\", \"decoder.up.0.block.2.conv1.weight\", \"decoder.up.0.block.2.conv1.bias\", \"decoder.up.0.block.2.norm2.weight\", \"decoder.up.0.block.2.norm2.bias\", \"decoder.up.0.block.2.conv2.weight\", \"decoder.up.0.block.2.conv2.bias\", \"decoder.up.1.block.2.norm1.weight\", \"decoder.up.1.block.2.norm1.bias\", \"decoder.up.1.block.2.conv1.weight\", \"decoder.up.1.block.2.conv1.bias\", \"decoder.up.1.block.2.norm2.weight\", \"decoder.up.1.block.2.norm2.bias\", \"decoder.up.1.block.2.conv2.weight\", \"decoder.up.1.block.2.conv2.bias\", \"decoder.up.2.block.0.nin_shortcut.weight\", \"decoder.up.2.block.0.nin_shortcut.bias\", \"decoder.up.2.block.2.norm1.weight\", \"decoder.up.2.block.2.norm1.bias\", \"decoder.up.2.block.2.conv1.weight\", \"decoder.up.2.block.2.conv1.bias\", \"decoder.up.2.block.2.norm2.weight\", \"decoder.up.2.block.2.norm2.bias\", \"decoder.up.2.block.2.conv2.weight\", \"decoder.up.2.block.2.conv2.bias\", \"decoder.up.3.block.0.norm1.weight\", \"decoder.up.3.block.0.norm1.bias\", \"decoder.up.3.block.0.conv1.weight\", \"decoder.up.3.block.0.conv1.bias\", \"decoder.up.3.block.0.norm2.weight\", \"decoder.up.3.block.0.norm2.bias\", \"decoder.up.3.block.0.conv2.weight\", \"decoder.up.3.block.0.conv2.bias\", \"decoder.up.3.block.1.norm1.weight\", \"decoder.up.3.block.1.norm1.bias\", \"decoder.up.3.block.1.conv1.weight\", \"decoder.up.3.block.1.conv1.bias\", \"decoder.up.3.block.1.norm2.weight\", \"decoder.up.3.block.1.norm2.bias\", \"decoder.up.3.block.1.conv2.weight\", \"decoder.up.3.block.1.conv2.bias\", \"decoder.up.3.block.2.norm1.weight\", \"decoder.up.3.block.2.norm1.bias\", \"decoder.up.3.block.2.conv1.weight\", \"decoder.up.3.block.2.conv1.bias\", \"decoder.up.3.block.2.norm2.weight\", \"decoder.up.3.block.2.norm2.bias\", \"decoder.up.3.block.2.conv2.weight\", \"decoder.up.3.block.2.conv2.bias\", \"decoder.up.3.upsample.conv.weight\", \"decoder.up.3.upsample.conv.bias\". \n\tsize mismatch for encoder.conv_in.weight: copying a param with shape torch.Size([32, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 1, 3, 3]).\n\tsize mismatch for encoder.conv_in.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for encoder.down.0.block.0.conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for encoder.down.0.block.0.conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.downsample.conv.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for encoder.down.0.downsample.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.1.block.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.1.block.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.1.block.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.nin_shortcut.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for encoder.down.1.block.0.nin_shortcut.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.downsample.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for encoder.down.1.downsample.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.2.block.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.2.block.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.2.block.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.mid.block_1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.q.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.k.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.v.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_2.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_2.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.norm_out.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.norm_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.conv_out.weight: copying a param with shape torch.Size([32, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 3, 3]).\n\tsize mismatch for encoder.conv_out.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.conv_in.weight: copying a param with shape torch.Size([128, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 128, 3, 3]).\n\tsize mismatch for decoder.conv_in.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.q.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.k.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.v.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_2.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_2.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.up.0.block.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.0.block.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.0.block.0.conv1.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for decoder.up.0.block.0.conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for decoder.up.0.block.0.conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.nin_shortcut.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for decoder.up.0.block.0.nin_shortcut.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for decoder.up.0.block.1.conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for decoder.up.0.block.1.conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.1.block.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.1.block.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.1.block.0.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.upsample.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.upsample.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.2.block.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.up.2.block.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.up.2.block.0.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.0.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.upsample.conv.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.upsample.conv.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.norm_out.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.norm_out.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.conv_out.weight: copying a param with shape torch.Size([1, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 128, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Initialize model (you can update the checkpoint path)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m model = \u001b[43mload_vanilla_vae_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_root\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/logs/checkpoints/chest_base_vae_quick-epoch=04-val/loss=0.040.ckpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Update this with your checkpoint path\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbase\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# \"base\" or \"beta\"\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mload_vanilla_vae_model\u001b[39m\u001b[34m(checkpoint_path, model_type)\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33mmodel.\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m             model_state_dict[key[\u001b[32m6\u001b[39m:]] = value  \u001b[38;5;66;03m# Remove \"model.\" prefix\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Model weights loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/TUDa/DGM/medvae-disentangled-multimodal/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for BaseVAE:\n\tMissing key(s) in state_dict: \"encoder.down.0.block.1.norm1.weight\", \"encoder.down.0.block.1.norm1.bias\", \"encoder.down.0.block.1.conv1.weight\", \"encoder.down.0.block.1.conv1.bias\", \"encoder.down.0.block.1.norm2.weight\", \"encoder.down.0.block.1.norm2.bias\", \"encoder.down.0.block.1.conv2.weight\", \"encoder.down.0.block.1.conv2.bias\", \"encoder.down.1.block.1.norm1.weight\", \"encoder.down.1.block.1.norm1.bias\", \"encoder.down.1.block.1.conv1.weight\", \"encoder.down.1.block.1.conv1.bias\", \"encoder.down.1.block.1.norm2.weight\", \"encoder.down.1.block.1.norm2.bias\", \"encoder.down.1.block.1.conv2.weight\", \"encoder.down.1.block.1.conv2.bias\", \"encoder.down.2.block.1.norm1.weight\", \"encoder.down.2.block.1.norm1.bias\", \"encoder.down.2.block.1.conv1.weight\", \"encoder.down.2.block.1.conv1.bias\", \"encoder.down.2.block.1.norm2.weight\", \"encoder.down.2.block.1.norm2.bias\", \"encoder.down.2.block.1.conv2.weight\", \"encoder.down.2.block.1.conv2.bias\", \"encoder.down.2.downsample.conv.weight\", \"encoder.down.2.downsample.conv.bias\", \"encoder.down.3.block.0.norm1.weight\", \"encoder.down.3.block.0.norm1.bias\", \"encoder.down.3.block.0.conv1.weight\", \"encoder.down.3.block.0.conv1.bias\", \"encoder.down.3.block.0.norm2.weight\", \"encoder.down.3.block.0.norm2.bias\", \"encoder.down.3.block.0.conv2.weight\", \"encoder.down.3.block.0.conv2.bias\", \"encoder.down.3.block.0.nin_shortcut.weight\", \"encoder.down.3.block.0.nin_shortcut.bias\", \"encoder.down.3.block.1.norm1.weight\", \"encoder.down.3.block.1.norm1.bias\", \"encoder.down.3.block.1.conv1.weight\", \"encoder.down.3.block.1.conv1.bias\", \"encoder.down.3.block.1.norm2.weight\", \"encoder.down.3.block.1.norm2.bias\", \"encoder.down.3.block.1.conv2.weight\", \"encoder.down.3.block.1.conv2.bias\", \"decoder.up.0.block.2.norm1.weight\", \"decoder.up.0.block.2.norm1.bias\", \"decoder.up.0.block.2.conv1.weight\", \"decoder.up.0.block.2.conv1.bias\", \"decoder.up.0.block.2.norm2.weight\", \"decoder.up.0.block.2.norm2.bias\", \"decoder.up.0.block.2.conv2.weight\", \"decoder.up.0.block.2.conv2.bias\", \"decoder.up.1.block.2.norm1.weight\", \"decoder.up.1.block.2.norm1.bias\", \"decoder.up.1.block.2.conv1.weight\", \"decoder.up.1.block.2.conv1.bias\", \"decoder.up.1.block.2.norm2.weight\", \"decoder.up.1.block.2.norm2.bias\", \"decoder.up.1.block.2.conv2.weight\", \"decoder.up.1.block.2.conv2.bias\", \"decoder.up.2.block.0.nin_shortcut.weight\", \"decoder.up.2.block.0.nin_shortcut.bias\", \"decoder.up.2.block.2.norm1.weight\", \"decoder.up.2.block.2.norm1.bias\", \"decoder.up.2.block.2.conv1.weight\", \"decoder.up.2.block.2.conv1.bias\", \"decoder.up.2.block.2.norm2.weight\", \"decoder.up.2.block.2.norm2.bias\", \"decoder.up.2.block.2.conv2.weight\", \"decoder.up.2.block.2.conv2.bias\", \"decoder.up.3.block.0.norm1.weight\", \"decoder.up.3.block.0.norm1.bias\", \"decoder.up.3.block.0.conv1.weight\", \"decoder.up.3.block.0.conv1.bias\", \"decoder.up.3.block.0.norm2.weight\", \"decoder.up.3.block.0.norm2.bias\", \"decoder.up.3.block.0.conv2.weight\", \"decoder.up.3.block.0.conv2.bias\", \"decoder.up.3.block.1.norm1.weight\", \"decoder.up.3.block.1.norm1.bias\", \"decoder.up.3.block.1.conv1.weight\", \"decoder.up.3.block.1.conv1.bias\", \"decoder.up.3.block.1.norm2.weight\", \"decoder.up.3.block.1.norm2.bias\", \"decoder.up.3.block.1.conv2.weight\", \"decoder.up.3.block.1.conv2.bias\", \"decoder.up.3.block.2.norm1.weight\", \"decoder.up.3.block.2.norm1.bias\", \"decoder.up.3.block.2.conv1.weight\", \"decoder.up.3.block.2.conv1.bias\", \"decoder.up.3.block.2.norm2.weight\", \"decoder.up.3.block.2.norm2.bias\", \"decoder.up.3.block.2.conv2.weight\", \"decoder.up.3.block.2.conv2.bias\", \"decoder.up.3.upsample.conv.weight\", \"decoder.up.3.upsample.conv.bias\". \n\tsize mismatch for encoder.conv_in.weight: copying a param with shape torch.Size([32, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 1, 3, 3]).\n\tsize mismatch for encoder.conv_in.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for encoder.down.0.block.0.conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.block.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for encoder.down.0.block.0.conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.0.downsample.conv.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for encoder.down.0.downsample.conv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.1.block.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.1.block.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.down.1.block.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for encoder.down.1.block.0.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.block.0.nin_shortcut.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for encoder.down.1.block.0.nin_shortcut.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.1.downsample.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for encoder.down.1.downsample.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.2.block.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.2.block.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.down.2.block.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for encoder.down.2.block.0.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for encoder.down.2.block.0.nin_shortcut.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.mid.block_1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.q.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.k.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.v.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.attn_1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for encoder.mid.attn_1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_2.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.mid.block_2.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for encoder.mid.block_2.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.norm_out.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.norm_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for encoder.conv_out.weight: copying a param with shape torch.Size([32, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 3, 3]).\n\tsize mismatch for encoder.conv_out.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.conv_in.weight: copying a param with shape torch.Size([128, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 128, 3, 3]).\n\tsize mismatch for decoder.conv_in.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.q.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.q.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.k.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.k.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.v.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.v.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.attn_1.proj_out.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 1, 1]).\n\tsize mismatch for decoder.mid.attn_1.proj_out.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_2.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.mid.block_2.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for decoder.mid.block_2.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.up.0.block.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.0.block.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.0.block.0.conv1.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for decoder.up.0.block.0.conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for decoder.up.0.block.0.conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.0.nin_shortcut.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for decoder.up.0.block.0.nin_shortcut.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for decoder.up.0.block.1.conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.0.block.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for decoder.up.0.block.1.conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.up.1.block.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.1.block.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.1.block.0.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.block.0.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for decoder.up.1.block.0.nin_shortcut.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.block.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.block.1.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.1.upsample.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for decoder.up.1.upsample.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.up.2.block.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.up.2.block.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.up.2.block.0.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.0.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.0.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.0.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.block.0.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.block.1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.block.1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.up.2.upsample.conv.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decoder.up.2.upsample.conv.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.norm_out.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.norm_out.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.conv_out.weight: copying a param with shape torch.Size([1, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 128, 3, 3])."
     ]
    }
   ],
   "source": [
    "def load_vanilla_vae_model(checkpoint_path=None, model_type=\"base\"):\n",
    "    \"\"\"Load a vanilla VAE model with optional checkpoint weights.\"\"\"\n",
    "    \n",
    "    if model_type == \"base\":\n",
    "        model = BaseVAE(**MODEL_CONFIG)\n",
    "    elif model_type == \"beta\":\n",
    "        model = BetaVAE(beta=4.0, **MODEL_CONFIG)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"ðŸ“‚ Loading checkpoint from: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Extract model weights from Lightning checkpoint\n",
    "        model_state_dict = {}\n",
    "        for key, value in checkpoint[\"state_dict\"].items():\n",
    "            if key.startswith(\"model.\"):\n",
    "                model_state_dict[key[6:]] = value  # Remove \"model.\" prefix\n",
    "        \n",
    "        model.load_state_dict(model_state_dict)\n",
    "        print(\"âœ… Model weights loaded successfully!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No checkpoint provided - using randomly initialized weights\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"ðŸ§  Model loaded: {model.__class__.__name__}\")\n",
    "    print(f\"ðŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model (you can update the checkpoint path)\n",
    "model = load_vanilla_vae_model(\n",
    "    checkpoint_path=f'{project_root}/logs/checkpoints/chest_base_vae_quick-epoch=04-val/loss=0.040.ckpt',  # Update this with your checkpoint path\n",
    "    model_type=\"base\"      # \"base\" or \"beta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3e311",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name=\"chestmnist\", batch_size=32):\n",
    "    \"\"\"Load a specific MedMNIST dataset.\"\"\"\n",
    "    \n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not available. Choose from: {list(DATASETS.keys())}\")\n",
    "    \n",
    "    print(f\"ðŸ“Š Loading {DATASETS[dataset_name]['name']} dataset...\")\n",
    "    \n",
    "    # Create data module\n",
    "    datamodule = MedMNISTDataModule(\n",
    "        dataset_name=dataset_name,\n",
    "        data_dir=str(DATA_DIR),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Setup data\n",
    "    datamodule.setup()\n",
    "    \n",
    "    # Get dataloaders\n",
    "    train_loader = datamodule.train_dataloader()\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    \n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"ðŸ”¢ Train samples: {len(datamodule.train_dataset)}\")\n",
    "    print(f\"ðŸ”¢ Val samples: {len(datamodule.val_dataset)}\")\n",
    "    print(f\"ðŸ”¢ Test samples: {len(datamodule.test_dataset)}\")\n",
    "    \n",
    "    return datamodule, train_loader, val_loader, test_loader\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = \"chestmnist\"  # Change this to \"pneumoniamnist\" if you prefer\n",
    "datamodule, train_loader, val_loader, test_loader = load_dataset(dataset_name)\n",
    "\n",
    "# Get a sample batch for exploration\n",
    "sample_batch = next(iter(val_loader))\n",
    "if len(sample_batch) >= 2:\n",
    "    sample_images, sample_labels = sample_batch[0], sample_batch[1]\n",
    "    print(f\"ðŸ–¼ï¸ Sample batch shape: {sample_images.shape}\")\n",
    "    print(f\"ðŸ·ï¸ Sample labels shape: {sample_labels.shape}\")\n",
    "else:\n",
    "    sample_images = sample_batch\n",
    "    sample_labels = None\n",
    "    print(f\"ðŸ–¼ï¸ Sample batch shape: {sample_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac728a",
   "metadata": {},
   "source": [
    "## 5. Interactive Reconstruction Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(model, images):\n",
    "    \"\"\"Reconstruct images using the VAE model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        reconstructions = outputs[\"reconstruction\"]\n",
    "        return reconstructions.cpu()\n",
    "\n",
    "def plot_reconstruction_comparison(original, reconstructed, num_samples=8):\n",
    "    \"\"\"Plot original vs reconstructed images side by side.\"\"\"\n",
    "    num_samples = min(num_samples, original.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(2*num_samples, 4))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        img_orig = original[i].squeeze()\n",
    "        axes[0, i].imshow(img_orig, cmap='gray')\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstructed image\n",
    "        img_recon = reconstructed[i].squeeze()\n",
    "        axes[1, i].imshow(img_recon, cmap='gray')\n",
    "        axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive reconstruction widget\n",
    "@widgets.interact\n",
    "def interactive_reconstruction(\n",
    "    batch_index=widgets.IntSlider(min=0, max=9, step=1, value=0, description='Batch:'),\n",
    "    num_samples=widgets.IntSlider(min=1, max=8, step=1, value=4, description='Samples:')\n",
    "):\n",
    "    \"\"\"Interactive reconstruction interface.\"\"\"\n",
    "    \n",
    "    # Get a batch of images\n",
    "    val_iter = iter(val_loader)\n",
    "    for _ in range(batch_index + 1):\n",
    "        try:\n",
    "            batch = next(val_iter)\n",
    "        except StopIteration:\n",
    "            val_iter = iter(val_loader)\n",
    "            batch = next(val_iter)\n",
    "    \n",
    "    images = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "    \n",
    "    # Reconstruct images\n",
    "    reconstructions = reconstruct_images(model, images)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_reconstruction_comparison(images, reconstructions, num_samples)\n",
    "    \n",
    "    # Compute and display metrics\n",
    "    if len(images) > 0:\n",
    "        metrics = compute_reconstruction_metrics(images, reconstructions)\n",
    "        print(\"ðŸ“Š Reconstruction Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"ðŸŽ® Interactive reconstruction interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccfd8e",
   "metadata": {},
   "source": [
    "## 6. Image Generation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, num_samples=8, seed=None):\n",
    "    \"\"\"Generate new images from the latent space.\"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.sample(num_samples, device)\n",
    "        return generated.cpu()\n",
    "\n",
    "def plot_generated_images(images, title=\"Generated Images\"):\n",
    "    \"\"\"Plot a grid of generated images.\"\"\"\n",
    "    num_images = images.shape[0]\n",
    "    cols = min(4, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        img = images[i].squeeze()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Sample {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive generation widget\n",
    "@widgets.interact\n",
    "def interactive_generation(\n",
    "    num_samples=widgets.IntSlider(min=1, max=16, step=1, value=8, description='Samples:'),\n",
    "    seed=widgets.IntSlider(min=0, max=1000, step=1, value=42, description='Seed:'),\n",
    "    randomize=widgets.Checkbox(value=False, description='Random seed')\n",
    "):\n",
    "    \"\"\"Interactive generation interface.\"\"\"\n",
    "    \n",
    "    # Use random seed if requested\n",
    "    actual_seed = None if randomize else seed\n",
    "    \n",
    "    # Generate images\n",
    "    generated_images = generate_images(model, num_samples, actual_seed)\n",
    "    \n",
    "    # Plot generated images\n",
    "    plot_generated_images(generated_images, f\"Generated Images (seed: {'random' if randomize else seed})\")\n",
    "\n",
    "print(\"ðŸŽ¨ Interactive generation interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511c5ed",
   "metadata": {},
   "source": [
    "## 7. Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_in_latent_space(model, image1, image2, steps=8):\n",
    "    \"\"\"Interpolate between two images in latent space.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode both images\n",
    "        image1, image2 = image1.to(device), image2.to(device)\n",
    "        \n",
    "        mu1, logvar1 = model.encode(image1.unsqueeze(0))\n",
    "        mu2, logvar2 = model.encode(image2.unsqueeze(0))\n",
    "        \n",
    "        # Sample from the latent distributions\n",
    "        z1 = model.reparameterize(mu1, logvar1)\n",
    "        z2 = model.reparameterize(mu2, logvar2)\n",
    "        \n",
    "        # Interpolate\n",
    "        interpolations = []\n",
    "        for i in range(steps):\n",
    "            alpha = i / (steps - 1)\n",
    "            z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "            \n",
    "            # Decode interpolated latent\n",
    "            recon = model.decode(z_interp)\n",
    "            interpolations.append(recon.cpu())\n",
    "        \n",
    "        return torch.cat(interpolations, dim=0)\n",
    "\n",
    "def plot_interpolation(original1, original2, interpolations):\n",
    "    \"\"\"Plot interpolation sequence.\"\"\"\n",
    "    num_interp = len(interpolations)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_interp + 2, figsize=(2*(num_interp + 2), 3))\n",
    "    \n",
    "    # Plot first original\n",
    "    axes[0].imshow(original1.squeeze(), cmap='gray')\n",
    "    axes[0].set_title('Original 1')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot interpolations\n",
    "    for i, img in enumerate(interpolations):\n",
    "        axes[i + 1].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i + 1].set_title(f'Step {i + 1}')\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    # Plot second original\n",
    "    axes[-1].imshow(original2.squeeze(), cmap='gray')\n",
    "    axes[-1].set_title('Original 2')\n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Latent Space Interpolation', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive latent interpolation\n",
    "@widgets.interact\n",
    "def interactive_interpolation(\n",
    "    image1_idx=widgets.IntSlider(min=0, max=31, step=1, value=0, description='Image 1:'),\n",
    "    image2_idx=widgets.IntSlider(min=0, max=31, step=1, value=15, description='Image 2:'),\n",
    "    steps=widgets.IntSlider(min=3, max=10, step=1, value=6, description='Steps:')\n",
    "):\n",
    "    \"\"\"Interactive latent space interpolation.\"\"\"\n",
    "    \n",
    "    # Get sample images\n",
    "    images = sample_images[:32]  # Use first 32 images\n",
    "    \n",
    "    if image1_idx >= len(images) or image2_idx >= len(images):\n",
    "        print(\"âŒ Image index out of range!\")\n",
    "        return\n",
    "    \n",
    "    image1 = images[image1_idx]\n",
    "    image2 = images[image2_idx]\n",
    "    \n",
    "    # Perform interpolation\n",
    "    interpolations = interpolate_in_latent_space(model, image1, image2, steps)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_interpolation(image1, image2, interpolations)\n",
    "\n",
    "print(\"ðŸ”€ Interactive latent space interpolation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e1e1c",
   "metadata": {},
   "source": [
    "## 8. Summary & Tips\n",
    "\n",
    "### ðŸŽ¯ What you can do with this notebook:\n",
    "\n",
    "1. **ðŸ”„ Reconstruct images**: Use the interactive widget to see how well your VAE reconstructs real medical images\n",
    "2. **ðŸŽ¨ Generate new images**: Sample from the latent space to create entirely new synthetic medical images  \n",
    "3. **ðŸ”€ Explore latent space**: Interpolate between images to understand the learned representations\n",
    "4. **ðŸ“Š Analyze performance**: View reconstruction metrics like MSE, PSNR, and SSIM\n",
    "\n",
    "### ðŸ’¡ Tips for better results:\n",
    "\n",
    "- **Load trained weights**: Update the checkpoint path in the model loading section to use your trained model\n",
    "- **Try different seeds**: Use the randomize option to explore diverse generations\n",
    "- **Experiment with interpolation**: Choose different image pairs to see interesting transitions\n",
    "- **Monitor metrics**: Lower MSE and higher PSNR/SSIM indicate better reconstructions\n",
    "\n",
    "### ðŸ”§ Customization options:\n",
    "\n",
    "- Change `dataset_name` to \"pneumoniamnist\" for different medical images\n",
    "- Modify `MODEL_CONFIG` to match your trained model architecture\n",
    "- Adjust `batch_size` and `num_samples` based on your system capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Happy exploring with your Vanilla VAE!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medmnist-conditional-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
