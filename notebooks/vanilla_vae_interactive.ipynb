{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566368e1",
   "metadata": {},
   "source": [
    "# Vanilla VAE Interactive Notebook ğŸ§ ğŸ“Š\n",
    "\n",
    "This notebook provides an interactive interface for exploring a vanilla Variational Autoencoder (VAE) trained on medical imaging data.\n",
    "\n",
    "## Features:\n",
    "- ğŸ”„ **Reconstruct samples** from the dataset\n",
    "- ğŸ¨ **Generate new images** from the latent space  \n",
    "- ğŸ“Š **Interactive visualizations** with widgets\n",
    "- ğŸ” **Compare** original vs reconstructed images\n",
    "- ğŸ¯ **Single modality** focus for detailed analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17608ec",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c90500ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Notebook directory: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal/notebooks\n",
      "ğŸ“ Project root: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal\n",
      "âœ… All libraries imported successfully!\n",
      "ğŸ”§ PyTorch version: 2.7.1\n",
      "ğŸ¯ Device available: CPU\n",
      "âœ… All libraries imported successfully!\n",
      "ğŸ”§ PyTorch version: 2.7.1\n",
      "ğŸ¯ Device available: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the project root to the Python path\n",
    "# Get the notebook directory and go up one level to the project root\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent  # Go up one level from notebooks/ to project root\n",
    "print(f\"ğŸ“ Notebook directory: {notebook_dir}\")\n",
    "print(f\"ğŸ“ Project root: {project_root}\")\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"âœ… Added project root to Python path\")\n",
    "\n",
    "# Import project modules\n",
    "from src.models import BaseVAE, BetaVAE\n",
    "from src.data import MedMNISTDataModule  # Corrected import path\n",
    "from src.utils import compute_reconstruction_metrics\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ğŸ”§ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ¯ Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3a03f",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb4a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸  Using device: cpu\n",
      "âš™ï¸ Configuration loaded successfully!\n",
      "ğŸ“ Checkpoints directory: logs/checkpoints\n",
      "ğŸ“Š Available datasets: ['chestmnist', 'pneumoniamnist']\n",
      "ğŸ”§ Modern config: 128 hidden channels, 128 latent dim\n",
      "ğŸ”§ Legacy config: 32 hidden channels, 16 latent dim\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
    "\n",
    "# Model configuration for vanilla VAE\n",
    "MODEL_CONFIG = {\n",
    "    \"input_channels\": 1,      # Grayscale images (ChestMNIST)\n",
    "    \"latent_dim\": 128,        # Latent space dimension (modern)\n",
    "    \"hidden_channels\": 128,   # Hidden layer channels (modern)\n",
    "    \"resolution\": 28,         # Image resolution\n",
    "    \"ch_mult\": (1, 2, 4, 8),  # Channel multipliers (modern)\n",
    "    \"num_res_blocks\": 2,      # Number of residual blocks (modern)\n",
    "    \"attn_resolutions\": [16], # Attention at specific resolutions (modern)\n",
    "}\n",
    "\n",
    "# Legacy configuration (used by older checkpoints)\n",
    "LEGACY_CONFIG = {\n",
    "    \"input_channels\": 1,      # Grayscale images\n",
    "    \"latent_dim\": 16,         # Smaller latent space (becomes 32 with double_z=True)\n",
    "    \"hidden_channels\": 32,    # Fewer hidden channels\n",
    "    \"resolution\": 28,         # Image resolution\n",
    "    \"ch_mult\": (1, 2, 4),     # Fewer downsampling stages\n",
    "    \"num_res_blocks\": 1,      # Single ResNet block per stage\n",
    "    \"attn_resolutions\": [],   # No attention to save parameters\n",
    "    \"dropout\": 0.1,\n",
    "    \"use_linear_attn\": False,\n",
    "    \"attn_type\": \"vanilla\",\n",
    "    \"double_z\": True,\n",
    "}\n",
    "\n",
    "# Paths (update these based on your trained models)\n",
    "CHECKPOINTS_DIR = Path(\"logs/checkpoints\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Available datasets for vanilla VAE (single modality)\n",
    "DATASETS = {\n",
    "    \"chestmnist\": {\n",
    "        \"name\": \"ChestMNIST\", \n",
    "        \"channels\": 1, \n",
    "        \"description\": \"Chest X-Ray Images\"\n",
    "    },\n",
    "    \"pneumoniamnist\": {\n",
    "        \"name\": \"PneumoniaMNIST\", \n",
    "        \"channels\": 1, \n",
    "        \"description\": \"Pneumonia X-Ray Images\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Configuration loaded successfully!\")\n",
    "print(f\"ğŸ“ Checkpoints directory: {CHECKPOINTS_DIR}\")\n",
    "print(f\"ğŸ“Š Available datasets: {list(DATASETS.keys())}\")\n",
    "print(f\"ğŸ”§ Modern config: {MODEL_CONFIG['hidden_channels']} hidden channels, {MODEL_CONFIG['latent_dim']} latent dim\")\n",
    "print(f\"ğŸ”§ Legacy config: {LEGACY_CONFIG['hidden_channels']} hidden channels, {LEGACY_CONFIG['latent_dim']} latent dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438bc01",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4625595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading checkpoint from: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal/logs/checkpoints/chest_base_vae_quick-epoch=04-val/loss=0.040.ckpt\n",
      "âš ï¸ Architecture mismatch detected! Trying legacy architecture...\n",
      "âœ… Model weights loaded successfully with legacy architecture!\n",
      "ğŸ§  Model loaded: VanillaVAE\n",
      "ğŸ“Š Total parameters: 2,742,337\n"
     ]
    }
   ],
   "source": [
    "class VanillaVAE(BaseVAE):\n",
    "    \"\"\"\n",
    "    Vanilla VAE with the original architecture parameters.\n",
    "    This matches the older checkpoints that were saved with different parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # Override default parameters to match the original architecture\n",
    "        vanilla_config = {\n",
    "            \"input_channels\": 1,\n",
    "            \"latent_dim\": 16,       # This becomes 32 total with double_z=True (16*2)\n",
    "            \"hidden_channels\": 32,  # Original smaller architecture\n",
    "            \"ch_mult\": (1, 2, 4),   # Fewer downsampling stages\n",
    "            \"num_res_blocks\": 1,    # Single ResNet block per stage\n",
    "            \"attn_resolutions\": [], # No attention to save parameters\n",
    "            \"dropout\": 0.1,\n",
    "            \"resolution\": 28,       # Smaller resolution\n",
    "            \"use_linear_attn\": False,\n",
    "            \"attn_type\": \"vanilla\",\n",
    "            \"double_z\": True,       # This doubles the latent_dim output\n",
    "        }\n",
    "        # Update with any provided kwargs\n",
    "        vanilla_config.update(kwargs)\n",
    "        super().__init__(**vanilla_config)\n",
    "\n",
    "def load_vanilla_vae_model(checkpoint_path=None, model_type=\"base\", use_legacy=False):\n",
    "    \"\"\"Load a vanilla VAE model with optional checkpoint weights.\"\"\"\n",
    "    \n",
    "    if use_legacy:\n",
    "        # Use the original smaller architecture for legacy checkpoints\n",
    "        model = VanillaVAE()\n",
    "        print(\"ğŸ”„ Using legacy VanillaVAE architecture (hidden_channels=32, latent_dim=16)\")\n",
    "    elif model_type == \"base\":\n",
    "        model = BaseVAE(**MODEL_CONFIG)\n",
    "    elif model_type == \"beta\":\n",
    "        model = BetaVAE(beta=4.0, **MODEL_CONFIG)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"ğŸ“‚ Loading checkpoint from: {checkpoint_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "            \n",
    "            # Extract model weights from Lightning checkpoint\n",
    "            model_state_dict = {}\n",
    "            for key, value in checkpoint[\"state_dict\"].items():\n",
    "                if key.startswith(\"model.\"):\n",
    "                    model_state_dict[key[6:]] = value  # Remove \"model.\" prefix\n",
    "            \n",
    "            model.load_state_dict(model_state_dict)\n",
    "            print(\"âœ… Model weights loaded successfully!\")\n",
    "        except RuntimeError as e:\n",
    "            if \"size mismatch\" in str(e):\n",
    "                print(\"âš ï¸ Architecture mismatch detected! Trying legacy architecture...\")\n",
    "                # Try loading with legacy architecture\n",
    "                model = VanillaVAE()\n",
    "                model.to(device)  # Move to device before loading\n",
    "                \n",
    "                model_state_dict = {}\n",
    "                for key, value in checkpoint[\"state_dict\"].items():\n",
    "                    if key.startswith(\"model.\"):\n",
    "                        model_state_dict[key[6:]] = value\n",
    "                \n",
    "                model.load_state_dict(model_state_dict)\n",
    "                print(\"âœ… Model weights loaded successfully with legacy architecture!\")\n",
    "            else:\n",
    "                raise e\n",
    "    else:\n",
    "        print(\"âš ï¸ No checkpoint provided - using randomly initialized weights\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"ğŸ§  Model loaded: {model.__class__.__name__}\")\n",
    "    print(f\"ğŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model (you can update the checkpoint path)\n",
    "model = load_vanilla_vae_model(\n",
    "    checkpoint_path=f'{project_root}/logs/checkpoints/chest_base_vae_quick-epoch=04-val/loss=0.040.ckpt',  # Update this with your checkpoint path\n",
    "    model_type=\"base\",      # \"base\" or \"beta\"\n",
    "    use_legacy=False        # Set to True if you know it's a legacy checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3e311",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dd3b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loading ChestMNIST dataset...\n",
      "ğŸ“ Data path: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal/data\n",
      "Loading modality info for chestmnist...\n",
      "Loading modality info for chestmnist...\n",
      "  chestmnist: 1 channels\n",
      "  chestmnist: 1 channels\n",
      "âœ… Dataset loaded successfully!\n",
      "ğŸ”¢ Train samples: 78468\n",
      "ğŸ”¢ Val samples: 11219\n",
      "ğŸ”¢ Test samples: 22433\n",
      "âœ… Dataset loaded successfully!\n",
      "ğŸ”¢ Train samples: 78468\n",
      "ğŸ”¢ Val samples: 11219\n",
      "ğŸ”¢ Test samples: 22433\n",
      "ğŸ–¼ï¸ Sample batch shape: torch.Size([32, 1, 28, 28])\n",
      "ğŸ·ï¸ Sample labels shape: torch.Size([32, 1])\n",
      "ğŸ–¼ï¸ Sample batch shape: torch.Size([32, 1, 28, 28])\n",
      "ğŸ·ï¸ Sample labels shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_name=\"chestmnist\", batch_size=32):\n",
    "    \"\"\"Load a specific MedMNIST dataset.\"\"\"\n",
    "    \n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not available. Choose from: {list(DATASETS.keys())}\")\n",
    "    \n",
    "    print(f\"ğŸ“Š Loading {DATASETS[dataset_name]['name']} dataset...\")\n",
    "    \n",
    "    # Create data module - note that it expects a list of dataset names\n",
    "    data_path = project_root / \"data\"  # Use absolute path\n",
    "    print(f\"ğŸ“ Data path: {data_path}\")\n",
    "    \n",
    "    datamodule = MedMNISTDataModule(\n",
    "        dataset_names=[dataset_name],  # Pass as a list\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        size=28,  # Use 28x28 to match the model configuration\n",
    "        root=str(data_path),  # Use absolute path\n",
    "        normalize=True,\n",
    "        augment_train=False  # Disable augmentation for cleaner visualization\n",
    "    )\n",
    "    \n",
    "    # Setup data\n",
    "    datamodule.setup()\n",
    "    \n",
    "    # Get dataloaders\n",
    "    train_loader = datamodule.train_dataloader()\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    \n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"ğŸ”¢ Train samples: {len(datamodule.train_dataset)}\")\n",
    "    print(f\"ğŸ”¢ Val samples: {len(datamodule.val_dataset)}\")\n",
    "    print(f\"ğŸ”¢ Test samples: {len(datamodule.test_dataset)}\")\n",
    "    \n",
    "    return datamodule, train_loader, val_loader, test_loader\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = \"chestmnist\"  # Change this to \"pneumoniamnist\" if you prefer\n",
    "datamodule, train_loader, val_loader, test_loader = load_dataset(dataset_name)\n",
    "\n",
    "# Get a sample batch for exploration\n",
    "sample_batch = next(iter(val_loader))\n",
    "if len(sample_batch) >= 2:\n",
    "    sample_images, sample_labels = sample_batch[0], sample_batch[1]\n",
    "    print(f\"ğŸ–¼ï¸ Sample batch shape: {sample_images.shape}\")\n",
    "    print(f\"ğŸ·ï¸ Sample labels shape: {sample_labels.shape}\")\n",
    "else:\n",
    "    sample_images = sample_batch\n",
    "    sample_labels = None\n",
    "    print(f\"ğŸ–¼ï¸ Sample batch shape: {sample_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac728a",
   "metadata": {},
   "source": [
    "## 5. Interactive Reconstruction Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1e4f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1e6619a4a74874961ae16ed393f186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Batch:', max=9), IntSlider(value=4, description='Samplesâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ® Interactive reconstruction interface ready!\n"
     ]
    }
   ],
   "source": [
    "def reconstruct_images(model, images):\n",
    "    \"\"\"Reconstruct images using the VAE model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        reconstructions = outputs[\"reconstruction\"]\n",
    "        return reconstructions.cpu()\n",
    "\n",
    "def plot_reconstruction_comparison(original, reconstructed, num_samples=8):\n",
    "    \"\"\"Plot original vs reconstructed images side by side.\"\"\"\n",
    "    num_samples = min(num_samples, original.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(2*num_samples, 4))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        img_orig = original[i].squeeze()\n",
    "        axes[0, i].imshow(img_orig, cmap='gray')\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstructed image\n",
    "        img_recon = reconstructed[i].squeeze()\n",
    "        axes[1, i].imshow(img_recon, cmap='gray')\n",
    "        axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive reconstruction widget\n",
    "@widgets.interact\n",
    "def interactive_reconstruction(\n",
    "    batch_index=widgets.IntSlider(min=0, max=9, step=1, value=0, description='Batch:'),\n",
    "    num_samples=widgets.IntSlider(min=1, max=8, step=1, value=4, description='Samples:')\n",
    "):\n",
    "    \"\"\"Interactive reconstruction interface.\"\"\"\n",
    "    \n",
    "    # Get a batch of images\n",
    "    val_iter = iter(val_loader)\n",
    "    for _ in range(batch_index + 1):\n",
    "        try:\n",
    "            batch = next(val_iter)\n",
    "        except StopIteration:\n",
    "            val_iter = iter(val_loader)\n",
    "            batch = next(val_iter)\n",
    "    \n",
    "    images = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "    \n",
    "    # Reconstruct images\n",
    "    reconstructions = reconstruct_images(model, images)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_reconstruction_comparison(images, reconstructions, num_samples)\n",
    "    \n",
    "    # Compute and display metrics\n",
    "    if len(images) > 0:\n",
    "        metrics = compute_reconstruction_metrics(images, reconstructions)\n",
    "        print(\"ğŸ“Š Reconstruction Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"ğŸ® Interactive reconstruction interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccfd8e",
   "metadata": {},
   "source": [
    "## 6. Image Generation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679c2fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2274866e886483a8062fcbf744fdd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=8, description='Samples:', max=16, min=1), IntSlider(value=42, descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ Interactive generation interface ready!\n"
     ]
    }
   ],
   "source": [
    "def generate_images(model, num_samples=8, seed=None):\n",
    "    \"\"\"Generate new images from the latent space.\"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.sample(num_samples, device)\n",
    "        return generated.cpu()\n",
    "\n",
    "def plot_generated_images(images, title=\"Generated Images\"):\n",
    "    \"\"\"Plot a grid of generated images.\"\"\"\n",
    "    num_images = images.shape[0]\n",
    "    cols = min(4, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        img = images[i].squeeze()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Sample {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive generation widget\n",
    "@widgets.interact\n",
    "def interactive_generation(\n",
    "    num_samples=widgets.IntSlider(min=1, max=16, step=1, value=8, description='Samples:'),\n",
    "    seed=widgets.IntSlider(min=0, max=1000, step=1, value=42, description='Seed:'),\n",
    "    randomize=widgets.Checkbox(value=False, description='Random seed')\n",
    "):\n",
    "    \"\"\"Interactive generation interface.\"\"\"\n",
    "    \n",
    "    # Use random seed if requested\n",
    "    actual_seed = None if randomize else seed\n",
    "    \n",
    "    # Generate images\n",
    "    generated_images = generate_images(model, num_samples, actual_seed)\n",
    "    \n",
    "    # Plot generated images\n",
    "    plot_generated_images(generated_images, f\"Generated Images (seed: {'random' if randomize else seed})\")\n",
    "\n",
    "print(\"ğŸ¨ Interactive generation interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511c5ed",
   "metadata": {},
   "source": [
    "## 7. Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c69dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2858f1eaeb40d9a1eb8bfe5076d409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Image 1:', max=31), IntSlider(value=15, description='Imaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”€ Interactive latent space interpolation ready!\n"
     ]
    }
   ],
   "source": [
    "def interpolate_in_latent_space(model, image1, image2, steps=8):\n",
    "    \"\"\"Interpolate between two images in latent space.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode both images\n",
    "        image1, image2 = image1.to(device), image2.to(device)\n",
    "        \n",
    "        mu1, logvar1 = model.encode(image1.unsqueeze(0))\n",
    "        mu2, logvar2 = model.encode(image2.unsqueeze(0))\n",
    "        \n",
    "        # Sample from the latent distributions\n",
    "        z1 = model.reparameterize(mu1, logvar1)\n",
    "        z2 = model.reparameterize(mu2, logvar2)\n",
    "        \n",
    "        # Interpolate\n",
    "        interpolations = []\n",
    "        for i in range(steps):\n",
    "            alpha = i / (steps - 1)\n",
    "            z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "            \n",
    "            # Decode interpolated latent\n",
    "            recon = model.decode(z_interp)\n",
    "            interpolations.append(recon.cpu())\n",
    "        \n",
    "        return torch.cat(interpolations, dim=0)\n",
    "\n",
    "def plot_interpolation(original1, original2, interpolations):\n",
    "    \"\"\"Plot interpolation sequence.\"\"\"\n",
    "    num_interp = len(interpolations)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_interp + 2, figsize=(2*(num_interp + 2), 3))\n",
    "    \n",
    "    # Plot first original\n",
    "    axes[0].imshow(original1.squeeze(), cmap='gray')\n",
    "    axes[0].set_title('Original 1')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot interpolations\n",
    "    for i, img in enumerate(interpolations):\n",
    "        axes[i + 1].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i + 1].set_title(f'Step {i + 1}')\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    # Plot second original\n",
    "    axes[-1].imshow(original2.squeeze(), cmap='gray')\n",
    "    axes[-1].set_title('Original 2')\n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Latent Space Interpolation', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive latent interpolation\n",
    "@widgets.interact\n",
    "def interactive_interpolation(\n",
    "    image1_idx=widgets.IntSlider(min=0, max=31, step=1, value=0, description='Image 1:'),\n",
    "    image2_idx=widgets.IntSlider(min=0, max=31, step=1, value=15, description='Image 2:'),\n",
    "    steps=widgets.IntSlider(min=3, max=10, step=1, value=6, description='Steps:')\n",
    "):\n",
    "    \"\"\"Interactive latent space interpolation.\"\"\"\n",
    "    \n",
    "    # Get sample images\n",
    "    images = sample_images[:32]  # Use first 32 images\n",
    "    \n",
    "    if image1_idx >= len(images) or image2_idx >= len(images):\n",
    "        print(\"âŒ Image index out of range!\")\n",
    "        return\n",
    "    \n",
    "    image1 = images[image1_idx]\n",
    "    image2 = images[image2_idx]\n",
    "    \n",
    "    # Perform interpolation\n",
    "    interpolations = interpolate_in_latent_space(model, image1, image2, steps)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_interpolation(image1, image2, interpolations)\n",
    "\n",
    "print(\"ğŸ”€ Interactive latent space interpolation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e1e1c",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### ğŸ¯ What you can do with this notebook:\n",
    "\n",
    "1. **ğŸ”„ Reconstruct images**: Use the interactive widget to see how well your VAE reconstructs real medical images\n",
    "2. **ğŸ¨ Generate new images**: Sample from the latent space to create entirely new synthetic medical images  \n",
    "3. **ğŸ”€ Explore latent space**: Interpolate between images to understand the learned representations\n",
    "4. **ğŸ“Š Analyze performance**: View reconstruction metrics like MSE, PSNR, and SSIM\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medmnist-conditional-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
