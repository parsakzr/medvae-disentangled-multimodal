{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8dd132",
   "metadata": {},
   "source": [
    "# Conditional Disentangled VAE Interactive Notebook üß†üîÄüìä\n",
    "\n",
    "This notebook provides an interactive interface for exploring a **Conditional Disentangled VAE** trained on multi-modal medical imaging data.\n",
    "\n",
    "## Advanced Features:\n",
    "- üéØ **Multi-modal reconstruction** across different medical imaging types\n",
    "- üîÑ **Conditional generation** by modality (ChestMNIST, PathMNIST, etc.)\n",
    "- üé® **Class-conditional generation** within each modality\n",
    "- üîÄ **Disentangled latent space** exploration (shared vs modality-specific)\n",
    "- üìä **Cross-modal comparisons** and analysis\n",
    "- üß™ **Modality transfer** experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40278617",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89fcbb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Notebook directory: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal/notebooks\n",
      "üìÅ Project root: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal\n",
      "‚úÖ All libraries imported successfully!\n",
      "üîß PyTorch version: 2.7.1\n",
      "üéØ Device available: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the project root to the Python path\n",
    "# Get the notebook directory and go up one level to the project root\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent  # Go up one level from notebooks/ to project root\n",
    "print(f\"üìÅ Notebook directory: {notebook_dir}\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"‚úÖ Added project root to Python path\")\n",
    "\n",
    "# Import project modules\n",
    "from src.models import ConditionalVAE, DisentangledConditionalVAE\n",
    "from src.data import MedMNISTDataModule  # Use the existing data module\n",
    "from src.utils import compute_reconstruction_metrics\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf186cc",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76d4bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cpu\n",
      "‚öôÔ∏è Configuration loaded successfully!\n",
      "üìÅ Checkpoints directory: logs/checkpoints\n",
      "üè• Available modalities: ['ChestMNIST', 'PathMNIST', 'OCTMNIST', 'PneumoniaMNIST', 'DermaMNIST']\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# Model configuration for Conditional/Disentangled VAE\n",
    "DISENTANGLED_CONFIG = {\n",
    "    \"num_modalities\": 5,\n",
    "    \"shared_latent_dim\": 8,\n",
    "    \"modality_latent_dim\": 8,\n",
    "    \"resolution\": 28,\n",
    "    \"hidden_channels\": 128,\n",
    "    \"ch_mult\": (1, 2, 4, 8),\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"attn_resolutions\": [16],\n",
    "    \"modality_separation_weight\": 0.1,\n",
    "    \"contrastive_weight\": 0.05,\n",
    "}\n",
    "\n",
    "CONDITIONAL_CONFIG = {\n",
    "    \"modalities\": [\"chestmnist\", \"pathmnist\", \"octmnist\", \"pneumoniamnist\", \"dermamnist\"],\n",
    "    \"condition_dim\": 5,\n",
    "    \"condition_method\": \"concat\",\n",
    "    \"input_channels\": 3,  # Max channels across modalities\n",
    "    \"latent_dim\": 128,\n",
    "    \"hidden_channels\": 128,\n",
    "    \"resolution\": 28,\n",
    "    \"ch_mult\": (1, 2, 4, 8),\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"attn_resolutions\": [16],\n",
    "}\n",
    "\n",
    "# Modality information\n",
    "MODALITIES = {\n",
    "    0: {\"name\": \"ChestMNIST\", \"channels\": 1, \"description\": \"Chest X-Ray Images\", \"classes\": [\"Normal\", \"Abnormal\"]},\n",
    "    1: {\"name\": \"PathMNIST\", \"channels\": 3, \"description\": \"Colon Pathology Images\", \"classes\": [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", \"STR\", \"TUM\"]},\n",
    "    2: {\"name\": \"OCTMNIST\", \"channels\": 3, \"description\": \"Retinal OCT Images\", \"classes\": [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]},\n",
    "    3: {\"name\": \"PneumoniaMNIST\", \"channels\": 1, \"description\": \"Pneumonia X-Ray Images\", \"classes\": [\"Normal\", \"Pneumonia\"]},\n",
    "    4: {\"name\": \"DermaMNIST\", \"channels\": 3, \"description\": \"Dermatoscope Images\", \"classes\": [\"ACK\", \"BCC\", \"MEL\", \"NEV\", \"PIH\", \"SEK\", \"UNK\"]},\n",
    "}\n",
    "\n",
    "# Paths\n",
    "CHECKPOINTS_DIR = Path(\"logs/checkpoints\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded successfully!\")\n",
    "print(f\"üìÅ Checkpoints directory: {CHECKPOINTS_DIR}\")\n",
    "print(f\"üè• Available modalities: {[info['name'] for info in MODALITIES.values()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a0048",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482bbd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69168272b09f4a3f8fb2c8a91749f0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Model Type:', options=(('Disentangled VAE', 'disentangled'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading checkpoint from: /Users/parsa/Projects/TUDa/DGM/medvae-disentangled-multimodal/logs/checkpoints/disentangled_multi_modal_cvae_quick_final.ckpt\n",
      "‚ö†Ô∏è Architecture mismatch detected! Trying legacy architecture...\n",
      "‚úÖ Model weights loaded successfully with legacy architecture!\n",
      "üß† Model loaded: LegacyDisentangledConditionalVAE\n",
      "üìä Total parameters: 2,744,671\n",
      "üîÄ Shared latent dim: 8\n",
      "üéØ Modality latent dim: 8\n",
      "üèóÔ∏è Hidden channels: 32\n"
     ]
    }
   ],
   "source": [
    "class LegacyDisentangledConditionalVAE(DisentangledConditionalVAE):\n",
    "    \"\"\"\n",
    "    Legacy DisentangledConditionalVAE with the original quick architecture parameters.\n",
    "    This matches the older checkpoints that were saved with different parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # Override default parameters to match the original quick architecture\n",
    "        legacy_config = {\n",
    "            \"num_modalities\": 5,\n",
    "            \"shared_latent_dim\": 8,\n",
    "            \"modality_latent_dim\": 8,\n",
    "            \"resolution\": 28,\n",
    "            \"hidden_channels\": 32,     # Original smaller architecture\n",
    "            \"ch_mult\": (1, 2, 4),      # Fewer downsampling stages\n",
    "            \"num_res_blocks\": 1,       # Single ResNet block per stage\n",
    "            \"attn_resolutions\": [],    # No attention to save parameters\n",
    "            \"dropout\": 0.1,\n",
    "            \"use_linear_attn\": False,\n",
    "            \"attn_type\": \"vanilla\",\n",
    "            \"double_z\": True,\n",
    "            \"modality_separation_weight\": 0.1,\n",
    "            \"contrastive_weight\": 0.05,\n",
    "        }\n",
    "        # Update with any provided kwargs\n",
    "        legacy_config.update(kwargs)\n",
    "        super().__init__(**legacy_config)\n",
    "\n",
    "def load_conditional_model(checkpoint_path=None, model_type=\"disentangled\", use_legacy=False):\n",
    "    \"\"\"Load a conditional VAE model with optional checkpoint weights.\"\"\"\n",
    "    \n",
    "    if use_legacy:\n",
    "        # Use the original smaller architecture for legacy checkpoints\n",
    "        if model_type == \"disentangled\":\n",
    "            model = LegacyDisentangledConditionalVAE()\n",
    "            config_used = {\n",
    "                \"shared_latent_dim\": 8,\n",
    "                \"modality_latent_dim\": 8,\n",
    "                \"hidden_channels\": 32\n",
    "            }\n",
    "            print(\"üîÑ Using legacy DisentangledConditionalVAE architecture (hidden_channels=32)\")\n",
    "        else:\n",
    "            # For conditional VAE, we'd need a legacy version too if needed\n",
    "            model = ConditionalVAE(**CONDITIONAL_CONFIG)\n",
    "            config_used = CONDITIONAL_CONFIG\n",
    "    elif model_type == \"disentangled\":\n",
    "        model = DisentangledConditionalVAE(**DISENTANGLED_CONFIG)\n",
    "        config_used = DISENTANGLED_CONFIG\n",
    "    elif model_type == \"conditional\":\n",
    "        model = ConditionalVAE(**CONDITIONAL_CONFIG)\n",
    "        config_used = CONDITIONAL_CONFIG\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Choose 'disentangled' or 'conditional'\")\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"üìÇ Loading checkpoint from: {checkpoint_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "            \n",
    "            # Extract model weights from Lightning checkpoint\n",
    "            model_state_dict = {}\n",
    "            for key, value in checkpoint[\"state_dict\"].items():\n",
    "                if key.startswith(\"model.\"):\n",
    "                    model_state_dict[key[6:]] = value  # Remove \"model.\" prefix\n",
    "            \n",
    "            model.load_state_dict(model_state_dict)\n",
    "            print(\"‚úÖ Model weights loaded successfully!\")\n",
    "        except RuntimeError as e:\n",
    "            if \"size mismatch\" in str(e):\n",
    "                print(\"‚ö†Ô∏è Architecture mismatch detected! Trying legacy architecture...\")\n",
    "                # Try loading with legacy architecture\n",
    "                if model_type == \"disentangled\":\n",
    "                    model = LegacyDisentangledConditionalVAE()\n",
    "                    model.to(device)  # Move to device before loading\n",
    "                    \n",
    "                    model_state_dict = {}\n",
    "                    for key, value in checkpoint[\"state_dict\"].items():\n",
    "                        if key.startswith(\"model.\"):\n",
    "                            model_state_dict[key[6:]] = value\n",
    "                    \n",
    "                    model.load_state_dict(model_state_dict)\n",
    "                    print(\"‚úÖ Model weights loaded successfully with legacy architecture!\")\n",
    "                    \n",
    "                    config_used = {\n",
    "                        \"shared_latent_dim\": 8,\n",
    "                        \"modality_latent_dim\": 8,\n",
    "                        \"hidden_channels\": 32\n",
    "                    }\n",
    "                else:\n",
    "                    raise e\n",
    "            else:\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading weights: {e}\")\n",
    "            print(\"Using randomly initialized weights...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No checkpoint provided - using randomly initialized weights\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"üß† Model loaded: {model.__class__.__name__}\")\n",
    "    print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    if model_type == \"disentangled\":\n",
    "        print(f\"üîÄ Shared latent dim: {config_used.get('shared_latent_dim', 'N/A')}\")\n",
    "        print(f\"üéØ Modality latent dim: {config_used.get('modality_latent_dim', 'N/A')}\")\n",
    "        print(f\"üèóÔ∏è Hidden channels: {config_used.get('hidden_channels', 'N/A')}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model selection widget\n",
    "model_type_widget = widgets.Dropdown(\n",
    "    options=[('Disentangled VAE', 'disentangled'), ('Conditional VAE', 'conditional')],\n",
    "    value='disentangled',\n",
    "    description='Model Type:'\n",
    ")\n",
    "\n",
    "checkpoint_path_widget = widgets.Text(\n",
    "    value=f'{project_root}/logs/checkpoints/disentangled_multi_modal_cvae_quick_final.ckpt',  # Default path\n",
    "    placeholder='Path to checkpoint file (.ckpt)',\n",
    "    description='Checkpoint:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(description=\"Load Model\", button_style='primary')\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_load_clicked(b):\n",
    "    with output_widget:\n",
    "        clear_output()\n",
    "        global model, model_type\n",
    "        model_type = model_type_widget.value\n",
    "        checkpoint = checkpoint_path_widget.value if checkpoint_path_widget.value.strip() else None\n",
    "        model = load_conditional_model(checkpoint, model_type)\n",
    "\n",
    "load_button.on_click(on_load_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([model_type_widget, load_button]),\n",
    "    checkpoint_path_widget,\n",
    "    output_widget\n",
    "]))\n",
    "\n",
    "# Initialize with default model and checkpoint\n",
    "model_type = 'disentangled'\n",
    "checkpoint_path = f'{project_root}/logs/checkpoints/disentangled_multi_modal_cvae_quick_final.ckpt'\n",
    "model = load_conditional_model(checkpoint_path, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ff7c2",
   "metadata": {},
   "source": [
    "## 4. Load Multi-Modal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b15d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select datasets and configure multi-modal data loading:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebb11b0bc1c467fae783590eace413a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectMultiple(description='Datasets:', index=(0, 1, 2), layout=Layout(height='120px', width='3‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default datasets...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Available datasets\n",
    "DATASETS = {\n",
    "    'chestmnist': 'data/chestmnist.npz',\n",
    "    'pathmnist': 'data/pathmnist.npz', \n",
    "    'octmnist': 'data/octmnist.npz',\n",
    "    'pneumoniamnist': 'data/pneumoniamnist.npz',\n",
    "    'dermamnist': 'data/dermamnist.npz'\n",
    "}\n",
    "\n",
    "# Data loading widgets\n",
    "dataset_selection = widgets.SelectMultiple(\n",
    "    options=list(DATASETS.keys()),\n",
    "    value=['chestmnist', 'pathmnist', 'octmnist'],  # Default selection\n",
    "    description='Datasets:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px', height='120px')\n",
    ")\n",
    "\n",
    "batch_size_widget = widgets.IntSlider(\n",
    "    value=32,\n",
    "    min=8,\n",
    "    max=128,\n",
    "    step=8,\n",
    "    description='Batch Size:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_data_button = widgets.Button(\n",
    "    description='Load Selected Datasets',\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "# Output area\n",
    "data_output = widgets.Output()\n",
    "\n",
    "def load_multimodal_data(b=None):\n",
    "    with data_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            selected_datasets = list(dataset_selection.value)\n",
    "            if not selected_datasets:\n",
    "                print(\"Please select at least one dataset.\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Loading datasets: {selected_datasets}\")\n",
    "            \n",
    "            # Use the existing MedMNISTDataModule for multi-modal loading\n",
    "            data_path = project_root / \"data\"\n",
    "            \n",
    "            datamodule = MedMNISTDataModule(\n",
    "                dataset_names=selected_datasets,  # Pass multiple datasets\n",
    "                batch_size=batch_size_widget.value,\n",
    "                num_workers=2,\n",
    "                size=28,  # Use 28x28 to match model configuration\n",
    "                root=str(data_path),\n",
    "                normalize=True,\n",
    "                augment_train=False  # Disable augmentation for cleaner visualization\n",
    "            )\n",
    "            \n",
    "            # Setup data\n",
    "            datamodule.setup()\n",
    "            \n",
    "            # Get dataloaders\n",
    "            train_loader = datamodule.train_dataloader()\n",
    "            val_loader = datamodule.val_dataloader()\n",
    "            test_loader = datamodule.test_dataloader()\n",
    "            \n",
    "            # Store globally for use in other cells\n",
    "            global multimodal_datamodule, multimodal_train_loader, multimodal_val_loader, multimodal_test_loader, dataset_info\n",
    "            multimodal_datamodule = datamodule\n",
    "            multimodal_train_loader = train_loader\n",
    "            multimodal_val_loader = val_loader\n",
    "            multimodal_test_loader = test_loader\n",
    "            \n",
    "            dataset_info = {\n",
    "                'modalities': selected_datasets,\n",
    "                'modality_to_idx': {mod: idx for idx, mod in enumerate(selected_datasets)},\n",
    "                'total_train_samples': len(datamodule.train_dataset),\n",
    "                'total_val_samples': len(datamodule.val_dataset),\n",
    "                'total_test_samples': len(datamodule.test_dataset),\n",
    "                'batch_size': batch_size_widget.value\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n‚úÖ Successfully loaded multi-modal dataset!\")\n",
    "            print(f\"Train samples: {len(datamodule.train_dataset)}\")\n",
    "            print(f\"Val samples: {len(datamodule.val_dataset)}\")\n",
    "            print(f\"Test samples: {len(datamodule.test_dataset)}\")\n",
    "            print(f\"Modalities: {selected_datasets}\")\n",
    "            print(f\"Batch size: {batch_size_widget.value}\")\n",
    "            \n",
    "            # Test a sample batch\n",
    "            sample_batch = next(iter(val_loader))\n",
    "            if isinstance(sample_batch, (tuple, list)) and len(sample_batch) >= 4:\n",
    "                images, labels, modalities, modality_indices = sample_batch\n",
    "                print(f\"\\nüìä Sample batch info:\")\n",
    "                print(f\"Images shape: {images.shape}\")\n",
    "                print(f\"Labels shape: {labels.shape}\")\n",
    "                print(f\"Modalities shape: {modalities.shape}\")\n",
    "                print(f\"Modality indices shape: {modality_indices.shape}\")\n",
    "                print(f\"Unique modalities in batch: {torch.unique(modality_indices)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "load_data_button.on_click(load_multimodal_data)\n",
    "\n",
    "# Display widgets\n",
    "print(\"Select datasets and configure multi-modal data loading:\")\n",
    "display(widgets.VBox([\n",
    "    dataset_selection,\n",
    "    batch_size_widget,\n",
    "    load_data_button,\n",
    "    data_output\n",
    "]))\n",
    "\n",
    "# Auto-load with default settings\n",
    "print(\"Loading default datasets...\")\n",
    "load_multimodal_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351800ce",
   "metadata": {},
   "source": [
    "## Quick Test - Multi-Modal Reconstruction Demo\n",
    "\n",
    "Let's test the DisentangledConditionalVAE functionality with a simple example before using the interactive widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a13d21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing DisentangledConditionalVAE with multi-modal data...\n",
      "‚úÖ Multi-modal data is available!\n",
      "üìä Batch info:\n",
      "  Images shape: torch.Size([32, 1, 28, 28])\n",
      "  Labels shape: torch.Size([32, 1])\n",
      "  Modality indices shape: torch.Size([32])\n",
      "  Unique modalities in batch: [0]\n",
      "\n",
      "üîÑ Testing reconstruction...\n",
      "Input images shape: torch.Size([4, 1, 28, 28])\n",
      "Modality indices: [0, 0, 0, 0]\n",
      "Labels: [12, 0, 0, 2]\n",
      "Reconstructed images shape: torch.Size([4, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Reconstruction Metrics:\n",
      "  MSE Loss: 0.208506\n",
      "  Tested modalities: ['chestmnist', 'chestmnist', 'chestmnist', 'chestmnist']\n",
      "‚úÖ DisentangledConditionalVAE test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Quick test of multi-modal reconstruction functionality\n",
    "print(\"üß™ Testing DisentangledConditionalVAE with multi-modal data...\")\n",
    "\n",
    "# Wait for data loading to complete and check if data is available\n",
    "if 'multimodal_val_loader' in globals():\n",
    "    print(\"‚úÖ Multi-modal data is available!\")\n",
    "    \n",
    "    # Get a sample batch\n",
    "    try:\n",
    "        sample_batch = next(iter(multimodal_val_loader))\n",
    "        if isinstance(sample_batch, (tuple, list)) and len(sample_batch) >= 4:\n",
    "            images, labels, modality_names, modality_indices = sample_batch\n",
    "            print(f\"üìä Batch info:\")\n",
    "            print(f\"  Images shape: {images.shape}\")\n",
    "            print(f\"  Labels shape: {labels.shape}\")\n",
    "            print(f\"  Modality indices shape: {modality_indices.shape}\")\n",
    "            print(f\"  Unique modalities in batch: {torch.unique(modality_indices).tolist()}\")\n",
    "            \n",
    "            # Test reconstruction with the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Take first 4 samples for testing\n",
    "                test_images = images[:4].to(device)\n",
    "                test_modality_indices = modality_indices[:4].to(device)\n",
    "                test_labels = labels[:4].to(device)\n",
    "                \n",
    "                print(f\"\\nüîÑ Testing reconstruction...\")\n",
    "                print(f\"Input images shape: {test_images.shape}\")\n",
    "                print(f\"Modality indices: {test_modality_indices.tolist()}\")\n",
    "                print(f\"Labels: {test_labels.squeeze().tolist()}\")\n",
    "                \n",
    "                # Perform forward pass with correct parameters\n",
    "                # The DisentangledConditionalVAE forward method expects (x, modality_indices, return_latents)\n",
    "                outputs = model(test_images, test_modality_indices, return_latents=False)\n",
    "                reconstruction = outputs[\"reconstruction\"]\n",
    "                \n",
    "                print(f\"Reconstructed images shape: {reconstruction.shape}\")\n",
    "                \n",
    "                # Plot comparison\n",
    "                fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "                \n",
    "                for i in range(4):\n",
    "                    # Original image\n",
    "                    orig_img = test_images[i].cpu()\n",
    "                    if orig_img.shape[0] == 1:  # Grayscale\n",
    "                        orig_img = orig_img.squeeze()\n",
    "                        cmap = 'gray'\n",
    "                    else:  # RGB\n",
    "                        orig_img = orig_img.permute(1, 2, 0)\n",
    "                        cmap = None\n",
    "                    \n",
    "                    axes[0, i].imshow(orig_img, cmap=cmap)\n",
    "                    mod_name = dataset_info['modalities'][test_modality_indices[i].item()]\n",
    "                    axes[0, i].set_title(f'Original\\\\n{mod_name}')\n",
    "                    axes[0, i].axis('off')\n",
    "                    \n",
    "                    # Reconstructed image\n",
    "                    recon_img = reconstruction[i].cpu()\n",
    "                    if recon_img.shape[0] == 1:  # Grayscale\n",
    "                        recon_img = recon_img.squeeze()\n",
    "                        cmap = 'gray'\n",
    "                    else:  # RGB\n",
    "                        recon_img = recon_img.permute(1, 2, 0)\n",
    "                        cmap = None\n",
    "                    \n",
    "                    axes[1, i].imshow(recon_img, cmap=cmap)\n",
    "                    axes[1, i].set_title(f'Reconstructed\\\\n{mod_name}')\n",
    "                    axes[1, i].axis('off')\n",
    "                \n",
    "                plt.suptitle('DisentangledConditionalVAE - Multi-Modal Reconstruction Test', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Compute reconstruction metrics\n",
    "                mse_loss = torch.nn.functional.mse_loss(reconstruction, test_images)\n",
    "                print(f\"\\nüìä Reconstruction Metrics:\")\n",
    "                print(f\"  MSE Loss: {mse_loss.item():.6f}\")\n",
    "                print(f\"  Tested modalities: {[dataset_info['modalities'][idx.item()] for idx in test_modality_indices]}\")\n",
    "                \n",
    "                # Test latent space disentanglement\n",
    "                if 'shared_mu' in outputs and 'modality_mu' in outputs:\n",
    "                    shared_latents = outputs['shared_mu']\n",
    "                    modality_latents = outputs['modality_mu']\n",
    "                    print(f\"\\nüîÄ Disentanglement Info:\")\n",
    "                    print(f\"  Shared latents shape: {shared_latents.shape}\")\n",
    "                    print(f\"  Modality latents shape: {modality_latents.shape}\")\n",
    "                \n",
    "                print(\"‚úÖ DisentangledConditionalVAE test completed successfully!\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Unexpected batch format\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during testing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≥ Waiting for multi-modal data to load...\")\n",
    "    print(\"Please run the data loading cell first or wait for it to complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176bad6",
   "metadata": {},
   "source": [
    "## 5. Interactive Multi-Modal Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98daf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a sample to reconstruct:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d6a9be01f243688a7a929d8da8794b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Modality:', options=('chestmnist', 'pathmnist', 'octmnist'), style=Descri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def reconstruct_samples():\n",
    "    \"\"\"Interactive reconstruction interface\"\"\"\n",
    "    \n",
    "    # Sample selection widgets\n",
    "    modality_dropdown = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Modality:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    sample_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=99,\n",
    "        description='Sample Index:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Add data split selection\n",
    "    data_split_dropdown = widgets.Dropdown(\n",
    "        options=[('Training Data', 'train'), ('Validation Data', 'val')],\n",
    "        value='train',\n",
    "        description='Data Split:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    reconstruct_button = widgets.Button(\n",
    "        description='Reconstruct Sample',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    reconstruction_output = widgets.Output()\n",
    "    \n",
    "    def update_modality_options():\n",
    "        if 'dataset_info' in globals():\n",
    "            modality_dropdown.options = dataset_info['modalities']\n",
    "            if dataset_info['modalities']:\n",
    "                modality_dropdown.value = dataset_info['modalities'][0]\n",
    "    \n",
    "    def perform_reconstruction(b=None):\n",
    "        with reconstruction_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if 'model' not in globals():\n",
    "                print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            # Select the appropriate dataloader\n",
    "            data_split = data_split_dropdown.value\n",
    "            if data_split == 'train' and 'multimodal_train_loader' in globals():\n",
    "                dataloader = multimodal_train_loader\n",
    "                print(f\"üîç Using training data (better mixing)...\")\n",
    "            elif data_split == 'val' and 'multimodal_val_loader' in globals():\n",
    "                dataloader = multimodal_val_loader\n",
    "                print(f\"üîç Using validation data...\")\n",
    "            else:\n",
    "                print(\"‚ùå Please load dataset first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                modality_name = modality_dropdown.value\n",
    "                sample_idx = sample_slider.value\n",
    "                modality_idx = dataset_info['modality_to_idx'][modality_name]\n",
    "                \n",
    "                print(f\"Searching for {modality_name} samples (modality index: {modality_idx})...\")\n",
    "                \n",
    "                # Search through multiple batches to find the desired modality\n",
    "                max_batches_to_check = 50  # Increase search range\n",
    "                found_samples = False\n",
    "                \n",
    "                for batch_num in range(max_batches_to_check):\n",
    "                    try:\n",
    "                        # Get a new batch each time\n",
    "                        data_iter = iter(dataloader)\n",
    "                        for _ in range(batch_num + 1):  # Skip to the batch_num-th batch\n",
    "                            sample_batch = next(data_iter)\n",
    "                        \n",
    "                        if isinstance(sample_batch, (tuple, list)) and len(sample_batch) >= 4:\n",
    "                            images, labels, modality_names, modality_indices = sample_batch\n",
    "                            \n",
    "                            # Check if this batch contains our desired modality\n",
    "                            modality_mask = modality_indices == modality_idx\n",
    "                            modality_samples = images[modality_mask]\n",
    "                            modality_labels = labels[modality_mask]\n",
    "                            \n",
    "                            if len(modality_samples) > 0:\n",
    "                                print(f\"‚úÖ Found {len(modality_samples)} {modality_name} samples in batch {batch_num + 1}\")\n",
    "                                found_samples = True\n",
    "                                \n",
    "                                # Use the first available sample or the requested index if available\n",
    "                                actual_idx = min(sample_idx, len(modality_samples) - 1)\n",
    "                                image = modality_samples[actual_idx:actual_idx+1].to(device)  # Keep batch dimension\n",
    "                                class_condition = modality_labels[actual_idx:actual_idx+1].to(device)\n",
    "                                modality_condition = torch.tensor([modality_idx]).to(device)\n",
    "                                \n",
    "                                # Perform reconstruction\n",
    "                                model.eval()\n",
    "                                with torch.no_grad():\n",
    "                                    # Use the model's forward method\n",
    "                                    outputs = model(image, modality_condition, return_latents=False)\n",
    "                                    reconstruction = outputs['reconstruction']\n",
    "                                \n",
    "                                # Convert to numpy for visualization\n",
    "                                original_img = image.cpu().squeeze().numpy()\n",
    "                                reconstructed_img = reconstruction.cpu().squeeze().numpy()\n",
    "                                \n",
    "                                # Handle different channel configurations\n",
    "                                if original_img.ndim == 3 and original_img.shape[0] in [1, 3]:\n",
    "                                    original_img = np.transpose(original_img, (1, 2, 0))\n",
    "                                    reconstructed_img = np.transpose(reconstructed_img, (1, 2, 0))\n",
    "                                \n",
    "                                if original_img.shape[-1] == 1:\n",
    "                                    original_img = original_img.squeeze(-1)\n",
    "                                    reconstructed_img = reconstructed_img.squeeze(-1)\n",
    "                                \n",
    "                                # Create visualization\n",
    "                                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                                \n",
    "                                # Original image\n",
    "                                axes[0].imshow(original_img, cmap='gray' if len(original_img.shape) == 2 else None)\n",
    "                                axes[0].set_title(f'Original\\n{modality_name} - Class {class_condition.item()}')\n",
    "                                axes[0].axis('off')\n",
    "                                \n",
    "                                # Reconstructed image\n",
    "                                axes[1].imshow(reconstructed_img, cmap='gray' if len(reconstructed_img.shape) == 2 else None)\n",
    "                                axes[1].set_title(f'Reconstructed\\n{modality_name} - Class {class_condition.item()}')\n",
    "                                axes[1].axis('off')\n",
    "                                \n",
    "                                # Difference\n",
    "                                diff = np.abs(original_img - reconstructed_img)\n",
    "                                im = axes[2].imshow(diff, cmap='hot')\n",
    "                                axes[2].set_title('Absolute Difference')\n",
    "                                axes[2].axis('off')\n",
    "                                plt.colorbar(im, ax=axes[2])\n",
    "                                \n",
    "                                plt.tight_layout()\n",
    "                                plt.show()\n",
    "                                \n",
    "                                # Calculate metrics\n",
    "                                mse = F.mse_loss(reconstruction, image).item()\n",
    "                                print(f\"\\nüìä Reconstruction Metrics:\")\n",
    "                                print(f\"MSE Loss: {mse:.6f}\")\n",
    "                                print(f\"Modality: {modality_name} (index: {modality_idx})\")\n",
    "                                print(f\"Class: {class_condition.item()}\")\n",
    "                                print(f\"Sample used: {actual_idx} (requested: {sample_idx})\")\n",
    "                                print(f\"Found in batch: {batch_num + 1} of {data_split} data\")\n",
    "                                break\n",
    "                            else:\n",
    "                                # Only print every 10th batch to reduce noise\n",
    "                                if batch_num % 10 == 9:\n",
    "                                    unique_in_batch = torch.unique(modality_indices).tolist()\n",
    "                                    batch_modality_names = [dataset_info['modalities'][idx] for idx in unique_in_batch if idx < len(dataset_info['modalities'])]\n",
    "                                    print(f\"Batches {batch_num-8}-{batch_num + 1}: Contains {batch_modality_names}\")\n",
    "                                        \n",
    "                    except StopIteration:\n",
    "                        print(f\"Reached end of data after {batch_num + 1} batches\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing batch {batch_num + 1}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if not found_samples:\n",
    "                    print(f\"‚ùå No {modality_name} samples found in {max_batches_to_check} batches!\")\n",
    "                    print(\"üí° Try these solutions:\")\n",
    "                    print(\"   1. Switch between 'Training Data' and 'Validation Data'\")\n",
    "                    print(\"   2. Try a different modality (e.g., chestmnist)\")\n",
    "                    print(\"   3. The dataloader may group samples by modality\")\n",
    "                    \n",
    "                    # Show what modalities are actually available in recent batches\n",
    "                    print(f\"\\nüîç Checking what modalities are available in {data_split} data...\")\n",
    "                    available_modalities = set()\n",
    "                    for check_batch in range(min(10, max_batches_to_check)):\n",
    "                        try:\n",
    "                            data_iter = iter(dataloader)\n",
    "                            for _ in range(check_batch * 5 + 1):  # Sample every 5th batch\n",
    "                                check_sample = next(data_iter)\n",
    "                            if isinstance(check_sample, (tuple, list)) and len(check_sample) >= 4:\n",
    "                                _, _, _, check_modality_indices = check_sample\n",
    "                                batch_modalities = torch.unique(check_modality_indices).tolist()\n",
    "                                available_modalities.update(batch_modalities)\n",
    "                        except:\n",
    "                            break\n",
    "                    \n",
    "                    available_names = [dataset_info['modalities'][idx] for idx in available_modalities if idx < len(dataset_info['modalities'])]\n",
    "                    print(f\"üìä Modalities found in {data_split} data: {available_names}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during reconstruction: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    reconstruct_button.on_click(perform_reconstruction)\n",
    "    \n",
    "    # Update options when called\n",
    "    update_modality_options()\n",
    "    \n",
    "    # Display interface\n",
    "    print(\"Select a sample to reconstruct:\")\n",
    "    display(widgets.VBox([\n",
    "        modality_dropdown,\n",
    "        sample_slider,\n",
    "        data_split_dropdown,\n",
    "        reconstruct_button,\n",
    "        reconstruction_output\n",
    "    ]))\n",
    "\n",
    "# Call the function to create the interface\n",
    "reconstruct_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369701f",
   "metadata": {},
   "source": [
    "## 6. Conditional Generation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e9b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configure conditional generation parameters:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c099bed7644d3aa06c42d77b84e0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Target Modality:', options=('chestmnist', 'pathmnist', 'octmnist'), style‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def conditional_generation_interface():\n",
    "    \"\"\"Interactive conditional generation interface\"\"\"\n",
    "    \n",
    "    # Generation parameter widgets\n",
    "    gen_modality_dropdown = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Target Modality:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    gen_class_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=1,\n",
    "        description='Target Class:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    num_samples_gen = widgets.IntSlider(\n",
    "        value=8,\n",
    "        min=1,\n",
    "        max=16,\n",
    "        description='Number of Samples:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    temperature_slider = widgets.FloatSlider(\n",
    "        value=1.0,\n",
    "        min=0.1,\n",
    "        max=2.0,\n",
    "        step=0.1,\n",
    "        description='Temperature:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    generate_button = widgets.Button(\n",
    "        description='Generate Samples',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    generation_output = widgets.Output()\n",
    "    \n",
    "    def update_generation_options():\n",
    "        if 'dataset_info' in globals():\n",
    "            gen_modality_dropdown.options = dataset_info['modalities']\n",
    "            if dataset_info['modalities']:\n",
    "                gen_modality_dropdown.value = dataset_info['modalities'][0]\n",
    "                \n",
    "                # Update class range based on dataset (simplified for demo)\n",
    "                gen_class_slider.max = 15  # Most MedMNIST datasets have < 16 classes\n",
    "    \n",
    "    def generate_conditional_samples(b=None):\n",
    "        with generation_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if 'model' not in globals():\n",
    "                print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                modality_name = gen_modality_dropdown.value\n",
    "                target_class = gen_class_slider.value\n",
    "                n_samples = num_samples_gen.value\n",
    "                temperature = temperature_slider.value\n",
    "                \n",
    "                modality_idx = dataset_info['modality_to_idx'][modality_name]\n",
    "                \n",
    "                print(f\"üé® Generating {n_samples} samples...\")\n",
    "                print(f\"Modality: {modality_name} (index: {modality_idx})\")\n",
    "                print(f\"Class: {target_class}\")\n",
    "                print(f\"Temperature: {temperature}\")\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Get the actual latent dimensions from the model\n",
    "                    if hasattr(model, 'shared_latent_dim') and hasattr(model, 'modality_latent_dim'):\n",
    "                        shared_dim = model.shared_latent_dim\n",
    "                        modality_dim = model.modality_latent_dim\n",
    "                        total_latent_dim = shared_dim + modality_dim\n",
    "                    else:\n",
    "                        total_latent_dim = 16  # Default for legacy model\n",
    "                    \n",
    "                    # Sample from prior in the latent space\n",
    "                    z = torch.randn(n_samples, total_latent_dim).to(device) * temperature\n",
    "                    \n",
    "                    # Create condition tensors\n",
    "                    modality_condition = torch.full((n_samples,), modality_idx).to(device)\n",
    "                    \n",
    "                    # For generation, we can use the model's sample method or decode method\n",
    "                    if hasattr(model, 'sample'):\n",
    "                        # Use sample method if available\n",
    "                        generated_samples = model.sample(n_samples, device)\n",
    "                    elif hasattr(model, 'decode'):\n",
    "                        # Use decode method with latent samples\n",
    "                        generated_samples = model.decode(z, modality_condition)\n",
    "                    else:\n",
    "                        # Fallback: generate using reparameterization\n",
    "                        # Create dummy input to get through the model\n",
    "                        dummy_input = torch.zeros(n_samples, 1, 28, 28).to(device)\n",
    "                        outputs = model(dummy_input, modality_condition, return_latents=True)\n",
    "                        \n",
    "                        # Sample new latents and decode\n",
    "                        mu = outputs.get('mu', torch.zeros_like(z))\n",
    "                        logvar = outputs.get('logvar', torch.zeros_like(z))\n",
    "                        z_new = model.reparameterize(mu * 0 + z, logvar * 0 - 10)  # Use our random z\n",
    "                        generated_samples = model.decode(z_new, modality_condition)\n",
    "                    \n",
    "                    # Convert to numpy\n",
    "                    samples = generated_samples.cpu().numpy()\n",
    "                    \n",
    "                    # Create visualization grid\n",
    "                    cols = min(4, n_samples)\n",
    "                    rows = (n_samples + cols - 1) // cols\n",
    "                    \n",
    "                    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "                    if rows == 1:\n",
    "                        axes = axes.reshape(1, -1)\n",
    "                    if cols == 1:\n",
    "                        axes = axes.reshape(-1, 1)\n",
    "                    \n",
    "                    for i in range(n_samples):\n",
    "                        row = i // cols\n",
    "                        col = i % cols\n",
    "                        \n",
    "                        # Handle different channel configurations\n",
    "                        img = samples[i]\n",
    "                        if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                            img = np.transpose(img, (1, 2, 0))\n",
    "                        if img.shape[-1] == 1:\n",
    "                            img = img.squeeze(-1)\n",
    "                        \n",
    "                        axes[row, col].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                        axes[row, col].set_title(f'{modality_name}\\nClass {target_class}')\n",
    "                        axes[row, col].axis('off')\n",
    "                    \n",
    "                    # Hide empty subplots\n",
    "                    for i in range(n_samples, rows * cols):\n",
    "                        row = i // cols\n",
    "                        col = i % cols\n",
    "                        axes[row, col].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(f\"\\n‚úÖ Generated {n_samples} samples successfully!\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during generation: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    generate_button.on_click(generate_conditional_samples)\n",
    "    \n",
    "    # Update options when called\n",
    "    update_generation_options()\n",
    "    \n",
    "    # Display interface\n",
    "    print(\"Configure conditional generation parameters:\")\n",
    "    display(widgets.VBox([\n",
    "        gen_modality_dropdown,\n",
    "        gen_class_slider,\n",
    "        num_samples_gen,\n",
    "        temperature_slider,\n",
    "        generate_button,\n",
    "        generation_output\n",
    "    ]))\n",
    "\n",
    "# Call the function to create the interface\n",
    "conditional_generation_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117191a",
   "metadata": {},
   "source": [
    "## 7. Disentangled Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore the latent space:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32236919ee524d08bb126cb2231cb8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Target Modality:', options=('chestmnist', 'pathmnist', 'octmnist'), style‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def latent_space_exploration():\n",
    "    \"\"\"Interactive latent space exploration\"\"\"\n",
    "    \n",
    "    # Latent manipulation widgets\n",
    "    latent_sample_button = widgets.Button(\n",
    "        description='Sample New Point',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    encode_sample_button = widgets.Button(\n",
    "        description='Encode Random Sample',\n",
    "        button_style='warning'\n",
    "    )\n",
    "    \n",
    "    target_modality_dropdown = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Target Modality:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    exploration_output = widgets.Output()\n",
    "    \n",
    "    def update_exploration_options():\n",
    "        if 'dataset_info' in globals():\n",
    "            target_modality_dropdown.options = dataset_info['modalities']\n",
    "            if dataset_info['modalities']:\n",
    "                target_modality_dropdown.value = dataset_info['modalities'][0]\n",
    "    \n",
    "    def sample_latent_point(b=None):\n",
    "        with exploration_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if 'model' not in globals():\n",
    "                print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                modality_name = target_modality_dropdown.value\n",
    "                modality_idx = dataset_info['modality_to_idx'][modality_name]\n",
    "                \n",
    "                print(f\"üîç Exploring latent space for {modality_name}...\")\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Get the actual latent dimensions\n",
    "                    if hasattr(model, 'shared_latent_dim') and hasattr(model, 'modality_latent_dim'):\n",
    "                        shared_dim = model.shared_latent_dim\n",
    "                        modality_dim = model.modality_latent_dim\n",
    "                        total_latent_dim = shared_dim + modality_dim\n",
    "                    else:\n",
    "                        total_latent_dim = 16  # Default for legacy model\n",
    "                    \n",
    "                    # Create variations around a center point\n",
    "                    n_variations = 9\n",
    "                    \n",
    "                    # Instead of using decode directly, use the model's generation approach\n",
    "                    # Create dummy inputs with the right shape and use reparameterization\n",
    "                    dummy_input = torch.zeros(n_variations, 1, 28, 28).to(device)\n",
    "                    modality_condition = torch.full((n_variations,), modality_idx).to(device)\n",
    "                    \n",
    "                    # Generate different latent samples\n",
    "                    variations = []\n",
    "                    base_z = torch.randn(1, total_latent_dim).to(device)\n",
    "                    \n",
    "                    for i in range(3):\n",
    "                        for j in range(3):\n",
    "                            # Create variations by adding noise to the base latent\n",
    "                            noise_scale = 0.5 * (abs(i-1) + abs(j-1))  # More noise for corner points\n",
    "                            perturbation = torch.randn_like(base_z) * noise_scale\n",
    "                            z_var = base_z + perturbation\n",
    "                            variations.append(z_var)\n",
    "                    \n",
    "                    z_batch = torch.cat(variations, dim=0)\n",
    "                    \n",
    "                    # Method 1: Try using model's sample method if available\n",
    "                    if hasattr(model, 'sample') and callable(model.sample):\n",
    "                        try:\n",
    "                            generated_samples = model.sample(n_variations, device)\n",
    "                            print(\"‚úÖ Using model's sample method\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è Model sample method failed: {e}\")\n",
    "                            generated_samples = None\n",
    "                    else:\n",
    "                        generated_samples = None\n",
    "                    \n",
    "                    # Method 2: Use reparameterization trick through forward pass\n",
    "                    if generated_samples is None:\n",
    "                        try:\n",
    "                            # Get encoder outputs for structure\n",
    "                            encoder_outputs = model(dummy_input, modality_condition, return_latents=True)\n",
    "                            \n",
    "                            # Use the model's reparameterize method if available\n",
    "                            if hasattr(model, 'reparameterize'):\n",
    "                                # Create fake mu and logvar for our custom latents\n",
    "                                fake_mu = z_batch\n",
    "                                fake_logvar = torch.zeros_like(z_batch) - 2  # Low variance\n",
    "                                z_reparam = model.reparameterize(fake_mu, fake_logvar)\n",
    "                            else:\n",
    "                                z_reparam = z_batch\n",
    "                            \n",
    "                            # Try to decode using the model's decoder approach\n",
    "                            if hasattr(model, 'decoder') and hasattr(model.decoder, 'conv_in'):\n",
    "                                # Reshape latent for spatial decoder if needed\n",
    "                                # The decoder expects spatial input, so we need to reshape\n",
    "                                decoder_input_channels = model.decoder.conv_in.in_channels if hasattr(model.decoder, 'conv_in') else total_latent_dim\n",
    "                                \n",
    "                                if total_latent_dim != decoder_input_channels:\n",
    "                                    # Need to project to the right number of channels\n",
    "                                    if hasattr(model, 'latent_proj'):\n",
    "                                        z_proj = model.latent_proj(z_reparam)\n",
    "                                    else:\n",
    "                                        # Create a simple projection\n",
    "                                        z_proj = torch.nn.functional.linear(z_reparam, \n",
    "                                                                           torch.randn(decoder_input_channels, total_latent_dim).to(device))\n",
    "                                else:\n",
    "                                    z_proj = z_reparam\n",
    "                                \n",
    "                                # Reshape to spatial format (assuming 1x1 spatial size)\n",
    "                                z_spatial = z_proj.view(n_variations, -1, 1, 1)\n",
    "                                generated_samples = model.decoder(z_spatial)\n",
    "                            else:\n",
    "                                # Fallback: reconstruct by manipulating existing outputs\n",
    "                                original_recon = encoder_outputs.get('reconstruction', dummy_input)\n",
    "                                # Apply latent variations as perturbations to the reconstruction\n",
    "                                noise = torch.randn_like(original_recon) * 0.1\n",
    "                                generated_samples = original_recon + noise\n",
    "                            \n",
    "                            print(\"‚úÖ Using custom latent manipulation\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è Custom latent manipulation failed: {e}\")\n",
    "                            # Ultimate fallback: just add noise to dummy input\n",
    "                            noise = torch.randn_like(dummy_input) * 0.3\n",
    "                            generated_samples = torch.clamp(dummy_input + noise, 0, 1)\n",
    "                            print(\"üîÑ Using noise-based fallback\")\n",
    "                    \n",
    "                    # Convert to numpy for visualization\n",
    "                    variations_np = generated_samples.cpu().numpy()\n",
    "                    \n",
    "                    # Create visualization\n",
    "                    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "                    fig.suptitle(f'Latent Space Exploration - {modality_name}', fontsize=16)\n",
    "                    \n",
    "                    for i in range(3):\n",
    "                        for j in range(3):\n",
    "                            idx = i * 3 + j\n",
    "                            img = variations_np[idx]\n",
    "                            \n",
    "                            # Handle different channel configurations\n",
    "                            if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                                img = np.transpose(img, (1, 2, 0))\n",
    "                            if img.shape[-1] == 1:\n",
    "                                img = img.squeeze(-1)\n",
    "                            \n",
    "                            axes[i, j].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                            axes[i, j].set_title(f'Variation {idx+1}')\n",
    "                            axes[i, j].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(\"‚úÖ Latent space exploration complete!\")\n",
    "                    print(\"Each image shows a different point in the latent space with varying amounts of perturbation.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during latent exploration: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def encode_random_sample(b=None):\n",
    "        with exploration_output:\n",
    "            if 'model' not in globals():\n",
    "                print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            if 'multimodal_train_loader' not in globals():\n",
    "                print(\"‚ùå Please load training data first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                # Search for samples from different modalities using the same approach as reconstruction\n",
    "                print(\"üîç Searching for samples from different modalities...\")\n",
    "                \n",
    "                # Try to find samples from multiple modalities\n",
    "                found_modalities = {}\n",
    "                max_search_batches = 20\n",
    "                \n",
    "                for batch_num in range(max_search_batches):\n",
    "                    try:\n",
    "                        data_iter = iter(multimodal_train_loader)\n",
    "                        for _ in range(batch_num + 1):\n",
    "                            sample_batch = next(data_iter)\n",
    "                        \n",
    "                        if isinstance(sample_batch, tuple) and len(sample_batch) >= 4:\n",
    "                            images, labels, modality_names, modality_indices = sample_batch\n",
    "                            \n",
    "                            # Check what modalities are in this batch\n",
    "                            unique_modalities = torch.unique(modality_indices)\n",
    "                            \n",
    "                            for mod_idx in unique_modalities:\n",
    "                                if mod_idx.item() not in found_modalities and len(found_modalities) < 3:\n",
    "                                    modality_mask = modality_indices == mod_idx\n",
    "                                    if modality_mask.any():\n",
    "                                        sample_image = images[modality_mask][0:1].to(device)\n",
    "                                        found_modalities[mod_idx.item()] = sample_image\n",
    "                                        \n",
    "                            if len(found_modalities) >= 3:  # Found enough modalities\n",
    "                                break\n",
    "                                \n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                if not found_modalities:\n",
    "                    print(\"‚ùå Could not find any modality samples\")\n",
    "                    return\n",
    "                \n",
    "                print(f\"‚úÖ Found samples from {len(found_modalities)} modalities\")\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Show original samples and reconstructions\n",
    "                    n_modalities = len(found_modalities)\n",
    "                    fig, axes = plt.subplots(2, n_modalities, figsize=(n_modalities * 3, 6))\n",
    "                    if n_modalities == 1:\n",
    "                        axes = axes.reshape(2, 1)\n",
    "                    \n",
    "                    for idx, (modality_idx, sample_image) in enumerate(found_modalities.items()):\n",
    "                        modality_condition = torch.tensor([modality_idx]).to(device)\n",
    "                        \n",
    "                        # Show original\n",
    "                        img = sample_image[0].cpu().numpy()\n",
    "                        if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                            img = np.transpose(img, (1, 2, 0))\n",
    "                        if img.shape[-1] == 1:\n",
    "                            img = img.squeeze(-1)\n",
    "                        \n",
    "                        axes[0, idx].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                        modality_name = dataset_info['modalities'][modality_idx] if modality_idx < len(dataset_info['modalities']) else f\"Modality {modality_idx}\"\n",
    "                        axes[0, idx].set_title(f'Original\\n{modality_name}')\n",
    "                        axes[0, idx].axis('off')\n",
    "                        \n",
    "                        # Encode and reconstruct\n",
    "                        outputs = model(sample_image, modality_condition, return_latents=True)\n",
    "                        \n",
    "                        # Get reconstruction\n",
    "                        if 'reconstruction' in outputs:\n",
    "                            reconstruction = outputs['reconstruction']\n",
    "                        elif 'recon' in outputs:\n",
    "                            reconstruction = outputs['recon']\n",
    "                        else:\n",
    "                            reconstruction = sample_image  # Fallback\n",
    "                        \n",
    "                        # Show reconstruction\n",
    "                        recon_img = reconstruction[0].cpu().numpy()\n",
    "                        if recon_img.ndim == 3 and recon_img.shape[0] in [1, 3]:\n",
    "                            recon_img = np.transpose(recon_img, (1, 2, 0))\n",
    "                        if recon_img.shape[-1] == 1:\n",
    "                            recon_img = recon_img.squeeze(-1)\n",
    "                        \n",
    "                        axes[1, idx].imshow(recon_img, cmap='gray' if len(recon_img.shape) == 2 else None)\n",
    "                        axes[1, idx].set_title(f'Reconstructed\\n{modality_name}')\n",
    "                        axes[1, idx].axis('off')\n",
    "                        \n",
    "                        # Print latent info\n",
    "                        if 'mu' in outputs:\n",
    "                            latent_dim = outputs['mu'].shape[1]\n",
    "                            print(f\"Latent dimension for {modality_name}: {latent_dim}\")\n",
    "                        elif 'shared_mu' in outputs and 'modality_mu' in outputs:\n",
    "                            shared_dim = outputs['shared_mu'].shape[1]\n",
    "                            mod_dim = outputs['modality_mu'].shape[1]\n",
    "                            print(f\"Latent dims for {modality_name}: shared={shared_dim}, modality={mod_dim}\")\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(\"‚úÖ Multi-modal encoding and reconstruction completed!\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during encoding: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    latent_sample_button.on_click(sample_latent_point)\n",
    "    encode_sample_button.on_click(encode_random_sample)\n",
    "    \n",
    "    # Update options when called\n",
    "    update_exploration_options()\n",
    "    \n",
    "    # Display interface\n",
    "    print(\"Explore the latent space:\")\n",
    "    display(widgets.VBox([\n",
    "        target_modality_dropdown,\n",
    "        widgets.HBox([latent_sample_button, encode_sample_button]),\n",
    "        exploration_output\n",
    "    ]))\n",
    "\n",
    "# Call the function to create the interface\n",
    "latent_space_exploration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1988af",
   "metadata": {},
   "source": [
    "## 8. Model Analysis & Comparison\n",
    "\n",
    "This notebook provides comprehensive tools for exploring conditional and disentangled VAE models:\n",
    "\n",
    "### Features:\n",
    "- **Multi-Modal Data Loading**: Load and combine multiple MedMNIST datasets\n",
    "- **Interactive Reconstruction**: Compare original vs reconstructed images across modalities\n",
    "- **Conditional Generation**: Generate samples conditioned on modality and class\n",
    "- **Latent Space Exploration**: Interpolate, analyze dimensions, and transfer between modalities\n",
    "- **Disentanglement Analysis**: Explore how the latent space separates modalities and classes\n",
    "\n",
    "### Usage Tips:\n",
    "1. Start by loading a trained model using the model loading interface\n",
    "2. Load your desired datasets (recommend starting with 2-3 modalities)\n",
    "3. Use reconstruction to verify model quality\n",
    "4. Experiment with conditional generation across different modalities\n",
    "5. Explore latent space properties to understand disentanglement\n",
    "\n",
    "### Next Steps:\n",
    "- Compare with the vanilla VAE notebook for single-modality behavior\n",
    "- Experiment with different temperature settings for generation\n",
    "- Analyze cross-modal relationships in the latent space\n",
    "- Use dimension analysis to understand which latent dimensions control specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa1d984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debugging dataset and batch structure...\n",
      "\n",
      "üìä Dataset Info:\n",
      "  modalities: ['chestmnist', 'pathmnist', 'octmnist']\n",
      "  modality_to_idx: {'chestmnist': 0, 'pathmnist': 1, 'octmnist': 2}\n",
      "  total_train_samples: 265941\n",
      "  total_val_samples: 32055\n",
      "  total_test_samples: 30613\n",
      "  batch_size: 32\n",
      "\n",
      "üéØ Available modalities: ['chestmnist', 'pathmnist', 'octmnist']\n",
      "üîó Modality mapping: {'chestmnist': 0, 'pathmnist': 1, 'octmnist': 2}\n",
      "\n",
      "üîÑ Checking batch structure from validation loader...\n",
      "\n",
      "  Batch 1:\n",
      "    Images shape: torch.Size([32, 1, 28, 28])\n",
      "    Modality indices shape: torch.Size([32])\n",
      "    Unique modalities in batch: [0]\n",
      "      Modality 0 (chestmnist): 32 samples\n",
      "\n",
      "  Batch 2:\n",
      "    Images shape: torch.Size([32, 1, 28, 28])\n",
      "    Modality indices shape: torch.Size([32])\n",
      "    Unique modalities in batch: [0]\n",
      "      Modality 0 (chestmnist): 32 samples\n",
      "\n",
      "  Batch 3:\n",
      "    Images shape: torch.Size([32, 1, 28, 28])\n",
      "    Modality indices shape: torch.Size([32])\n",
      "    Unique modalities in batch: [0]\n",
      "      Modality 0 (chestmnist): 32 samples\n",
      "\n",
      "üí° The issue might be:\n",
      "   1. Only one modality is loaded in the current batches\n",
      "   2. The batch size is too small to include all modalities\n",
      "   3. Data distribution is uneven across modalities\n",
      "   4. The dataloader is not properly mixing modalities\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check dataset info and batch structure\n",
    "print(\"üîç Debugging dataset and batch structure...\")\n",
    "\n",
    "if 'dataset_info' in globals():\n",
    "    print(f\"\\nüìä Dataset Info:\")\n",
    "    for key, value in dataset_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Available modalities: {dataset_info['modalities']}\")\n",
    "    print(f\"üîó Modality mapping: {dataset_info['modality_to_idx']}\")\n",
    "else:\n",
    "    print(\"‚ùå dataset_info not found!\")\n",
    "\n",
    "if 'multimodal_val_loader' in globals():\n",
    "    print(f\"\\nüîÑ Checking batch structure from validation loader...\")\n",
    "    \n",
    "    # Get multiple batches to see the distribution\n",
    "    for batch_num in range(3):\n",
    "        try:\n",
    "            sample_batch = next(iter(multimodal_val_loader))\n",
    "            if isinstance(sample_batch, (tuple, list)) and len(sample_batch) >= 4:\n",
    "                images, labels, modality_names, modality_indices = sample_batch\n",
    "                \n",
    "                print(f\"\\n  Batch {batch_num + 1}:\")\n",
    "                print(f\"    Images shape: {images.shape}\")\n",
    "                print(f\"    Modality indices shape: {modality_indices.shape}\")\n",
    "                print(f\"    Unique modalities in batch: {torch.unique(modality_indices).tolist()}\")\n",
    "                \n",
    "                # Count samples per modality\n",
    "                for mod_idx in torch.unique(modality_indices):\n",
    "                    count = (modality_indices == mod_idx).sum().item()\n",
    "                    mod_name = dataset_info['modalities'][mod_idx] if mod_idx < len(dataset_info['modalities']) else f\"Unknown_{mod_idx}\"\n",
    "                    print(f\"      Modality {mod_idx} ({mod_name}): {count} samples\")\n",
    "            else:\n",
    "                print(f\"  Batch {batch_num + 1}: Unexpected format - {type(sample_batch)}, length: {len(sample_batch) if hasattr(sample_batch, '__len__') else 'N/A'}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error getting batch {batch_num + 1}: {e}\")\n",
    "            break\n",
    "else:\n",
    "    print(\"‚ùå multimodal_val_loader not found!\")\n",
    "\n",
    "print(f\"\\nüí° The issue might be:\")\n",
    "print(f\"   1. Only one modality is loaded in the current batches\")\n",
    "print(f\"   2. The batch size is too small to include all modalities\") \n",
    "print(f\"   3. Data distribution is uneven across modalities\")\n",
    "print(f\"   4. The dataloader is not properly mixing modalities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8616653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing modality search across batches...\n",
      "\n",
      "üîç Searching for chestmnist (index 0)...\n",
      "  ‚úÖ Found 32 chestmnist samples in batch 1\n",
      "\n",
      "üîç Searching for pathmnist (index 1)...\n",
      "  Batch 1: Contains ['chestmnist']\n",
      "  Batch 2: Contains ['chestmnist']\n",
      "  Batch 3: Contains ['chestmnist']\n",
      "  Batch 4: Contains ['chestmnist']\n",
      "  Batch 5: Contains ['chestmnist']\n",
      "  Batch 6: Contains ['chestmnist']\n",
      "  Batch 7: Contains ['chestmnist']\n",
      "  Batch 8: Contains ['chestmnist']\n",
      "  Batch 9: Contains ['chestmnist']\n",
      "  Batch 10: Contains ['chestmnist']\n",
      "  Batch 11: Contains ['chestmnist']\n",
      "  Batch 12: Contains ['chestmnist']\n",
      "  Batch 13: Contains ['chestmnist']\n",
      "  Batch 14: Contains ['chestmnist']\n",
      "  Batch 15: Contains ['chestmnist']\n",
      "  Batch 16: Contains ['chestmnist']\n",
      "  Batch 17: Contains ['chestmnist']\n",
      "  Batch 18: Contains ['chestmnist']\n",
      "  Batch 19: Contains ['chestmnist']\n",
      "  Batch 20: Contains ['chestmnist']\n",
      "  ‚ùå pathmnist not found in 20 batches\n",
      "\n",
      "üîç Searching for octmnist (index 2)...\n",
      "  Batch 1: Contains ['chestmnist']\n",
      "  Batch 2: Contains ['chestmnist']\n",
      "  Batch 3: Contains ['chestmnist']\n",
      "  Batch 4: Contains ['chestmnist']\n",
      "  Batch 5: Contains ['chestmnist']\n",
      "  Batch 6: Contains ['chestmnist']\n",
      "  Batch 7: Contains ['chestmnist']\n",
      "  Batch 8: Contains ['chestmnist']\n",
      "  Batch 9: Contains ['chestmnist']\n",
      "  Batch 10: Contains ['chestmnist']\n",
      "  Batch 11: Contains ['chestmnist']\n",
      "  Batch 12: Contains ['chestmnist']\n",
      "  Batch 13: Contains ['chestmnist']\n",
      "  Batch 14: Contains ['chestmnist']\n",
      "  Batch 15: Contains ['chestmnist']\n",
      "  Batch 16: Contains ['chestmnist']\n",
      "  Batch 17: Contains ['chestmnist']\n",
      "  Batch 18: Contains ['chestmnist']\n",
      "  Batch 19: Contains ['chestmnist']\n",
      "  Batch 20: Contains ['chestmnist']\n",
      "  ‚ùå octmnist not found in 20 batches\n",
      "\n",
      "üí° Summary:\n",
      "   - The dataloader appears to group samples by modality\n",
      "   - Different batches contain different modalities\n",
      "   - The new reconstruction function should work by searching across batches\n"
     ]
    }
   ],
   "source": [
    "# Test: Verify we can find samples from different modalities\n",
    "print(\"üß™ Testing modality search across batches...\")\n",
    "\n",
    "if 'dataset_info' in globals() and 'multimodal_val_loader' in globals():\n",
    "    modalities_to_test = ['chestmnist', 'pathmnist', 'octmnist']\n",
    "    max_batches_to_check = 20\n",
    "    \n",
    "    for modality_name in modalities_to_test:\n",
    "        modality_idx = dataset_info['modality_to_idx'][modality_name]\n",
    "        print(f\"\\nüîç Searching for {modality_name} (index {modality_idx})...\")\n",
    "        \n",
    "        found = False\n",
    "        for batch_num in range(max_batches_to_check):\n",
    "            try:\n",
    "                # Get a new batch each time\n",
    "                data_iter = iter(multimodal_val_loader)\n",
    "                for _ in range(batch_num + 1):  # Skip to the batch_num-th batch\n",
    "                    sample_batch = next(data_iter)\n",
    "                \n",
    "                if isinstance(sample_batch, (tuple, list)) and len(sample_batch) >= 4:\n",
    "                    images, labels, modality_names, modality_indices = sample_batch\n",
    "                    \n",
    "                    # Check if this batch contains our desired modality\n",
    "                    modality_mask = modality_indices == modality_idx\n",
    "                    count = modality_mask.sum().item()\n",
    "                    \n",
    "                    if count > 0:\n",
    "                        print(f\"  ‚úÖ Found {count} {modality_name} samples in batch {batch_num + 1}\")\n",
    "                        found = True\n",
    "                        break\n",
    "                    else:\n",
    "                        unique_in_batch = torch.unique(modality_indices).tolist()\n",
    "                        batch_modality_names = [dataset_info['modalities'][idx] for idx in unique_in_batch if idx < len(dataset_info['modalities'])]\n",
    "                        print(f\"  Batch {batch_num + 1}: Contains {batch_modality_names}\")\n",
    "                        \n",
    "            except StopIteration:\n",
    "                print(f\"  Reached end of data after {batch_num + 1} batches\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"  Error in batch {batch_num + 1}: {e}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"  ‚ùå {modality_name} not found in {max_batches_to_check} batches\")\n",
    "    \n",
    "    print(f\"\\nüí° Summary:\")\n",
    "    print(f\"   - The dataloader appears to group samples by modality\")\n",
    "    print(f\"   - Different batches contain different modalities\")\n",
    "    print(f\"   - The new reconstruction function should work by searching across batches\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Required variables not found. Please run the data loading cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0b5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Checking individual dataset sizes...\n",
      "\n",
      "chestmnist:\n",
      "  Train: 78,468\n",
      "  Val: 11,219\n",
      "  Test: 22,433\n",
      "  Total: 112,120\n",
      "  Image shape: (28, 28)\n",
      "  Image dtype: uint8\n",
      "\n",
      "pathmnist:\n",
      "  Train: 89,996\n",
      "  Val: 10,004\n",
      "  Test: 7,180\n",
      "  Total: 107,180\n",
      "  Image shape: (28, 28, 3)\n",
      "  Image dtype: uint8\n",
      "\n",
      "octmnist:\n",
      "  Train: 97,477\n",
      "  Val: 10,832\n",
      "  Test: 1,000\n",
      "  Total: 109,309\n",
      "  Image shape: (28, 28)\n",
      "  Image dtype: uint8\n",
      "\n",
      "üí° Analysis:\n",
      "   If ChestMNIST has significantly more samples, that explains the bias\n",
      "   The dataloader might be going through datasets sequentially\n",
      "   We need to either:\n",
      "   1. Use a balanced sampling strategy\n",
      "   2. Modify the dataloader to ensure mixing\n",
      "   3. Use stratified sampling across modalities\n"
     ]
    }
   ],
   "source": [
    "# Check individual dataset sizes to understand the imbalance\n",
    "print(\"üìä Checking individual dataset sizes...\")\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"../data\")  # Adjust path if needed\n",
    "\n",
    "for dataset_name in ['chestmnist', 'pathmnist', 'octmnist']:\n",
    "    npz_file = data_dir / f\"{dataset_name}.npz\"\n",
    "    if npz_file.exists():\n",
    "        data = np.load(npz_file)\n",
    "        \n",
    "        # Get split sizes\n",
    "        train_size = len(data['train_images']) if 'train_images' in data else 0\n",
    "        val_size = len(data['val_images']) if 'val_images' in data else 0\n",
    "        test_size = len(data['test_images']) if 'test_images' in data else 0\n",
    "        total_size = train_size + val_size + test_size\n",
    "        \n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        print(f\"  Train: {train_size:,}\")\n",
    "        print(f\"  Val: {val_size:,}\")\n",
    "        print(f\"  Test: {test_size:,}\")\n",
    "        print(f\"  Total: {total_size:,}\")\n",
    "        \n",
    "        # Check data shapes\n",
    "        if 'train_images' in data:\n",
    "            print(f\"  Image shape: {data['train_images'].shape[1:]}\")\n",
    "            print(f\"  Image dtype: {data['train_images'].dtype}\")\n",
    "    else:\n",
    "        print(f\"\\n{dataset_name}: File not found at {npz_file}\")\n",
    "\n",
    "print(f\"\\nüí° Analysis:\")\n",
    "print(f\"   If ChestMNIST has significantly more samples, that explains the bias\")\n",
    "print(f\"   The dataloader might be going through datasets sequentially\")\n",
    "print(f\"   We need to either:\")\n",
    "print(f\"   1. Use a balanced sampling strategy\")\n",
    "print(f\"   2. Modify the dataloader to ensure mixing\")\n",
    "print(f\"   3. Use stratified sampling across modalities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medmnist-conditional-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
