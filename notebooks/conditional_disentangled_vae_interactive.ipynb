{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8dd132",
   "metadata": {},
   "source": [
    "# Conditional Disentangled VAE Interactive Notebook üß†üîÄüìä\n",
    "\n",
    "This notebook provides an interactive interface for exploring a **Conditional Disentangled VAE** trained on multi-modal medical imaging data.\n",
    "\n",
    "## Advanced Features:\n",
    "- üéØ **Multi-modal reconstruction** across different medical imaging types\n",
    "- üîÑ **Conditional generation** by modality (ChestMNIST, PathMNIST, etc.)\n",
    "- üé® **Class-conditional generation** within each modality\n",
    "- üîÄ **Disentangled latent space** exploration (shared vs modality-specific)\n",
    "- üìä **Cross-modal comparisons** and analysis\n",
    "- üß™ **Modality transfer** experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40278617",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fcbb48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, clear_output\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path().resolve().parent  # Go up one level from notebooks to project root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.models import ConditionalVAE, DisentangledConditionalVAE\n",
    "from src.data.multi_modal_datamodule import MultiModalDataModule\n",
    "from src.utils import compute_reconstruction_metrics\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf186cc",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# Model configuration for Conditional/Disentangled VAE\n",
    "DISENTANGLED_CONFIG = {\n",
    "    \"num_modalities\": 5,\n",
    "    \"shared_latent_dim\": 8,\n",
    "    \"modality_latent_dim\": 8,\n",
    "    \"resolution\": 28,\n",
    "    \"hidden_channels\": 128,\n",
    "    \"ch_mult\": (1, 2, 4, 8),\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"attn_resolutions\": [16],\n",
    "    \"modality_separation_weight\": 0.1,\n",
    "    \"contrastive_weight\": 0.05,\n",
    "}\n",
    "\n",
    "CONDITIONAL_CONFIG = {\n",
    "    \"modalities\": [\"chestmnist\", \"pathmnist\", \"octmnist\", \"pneumoniamnist\", \"dermamnist\"],\n",
    "    \"condition_dim\": 5,\n",
    "    \"condition_method\": \"concat\",\n",
    "    \"input_channels\": 3,  # Max channels across modalities\n",
    "    \"latent_dim\": 128,\n",
    "    \"hidden_channels\": 128,\n",
    "    \"resolution\": 28,\n",
    "    \"ch_mult\": (1, 2, 4, 8),\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"attn_resolutions\": [16],\n",
    "}\n",
    "\n",
    "# Modality information\n",
    "MODALITIES = {\n",
    "    0: {\"name\": \"ChestMNIST\", \"channels\": 1, \"description\": \"Chest X-Ray Images\", \"classes\": [\"Normal\", \"Abnormal\"]},\n",
    "    1: {\"name\": \"PathMNIST\", \"channels\": 3, \"description\": \"Colon Pathology Images\", \"classes\": [\"ADI\", \"BACK\", \"DEB\", \"LYM\", \"MUC\", \"MUS\", \"NORM\", \"STR\", \"TUM\"]},\n",
    "    2: {\"name\": \"OCTMNIST\", \"channels\": 3, \"description\": \"Retinal OCT Images\", \"classes\": [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]},\n",
    "    3: {\"name\": \"PneumoniaMNIST\", \"channels\": 1, \"description\": \"Pneumonia X-Ray Images\", \"classes\": [\"Normal\", \"Pneumonia\"]},\n",
    "    4: {\"name\": \"DermaMNIST\", \"channels\": 3, \"description\": \"Dermatoscope Images\", \"classes\": [\"ACK\", \"BCC\", \"MEL\", \"NEV\", \"PIH\", \"SEK\", \"UNK\"]},\n",
    "}\n",
    "\n",
    "# Paths\n",
    "CHECKPOINTS_DIR = Path(\"logs/checkpoints\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded successfully!\")\n",
    "print(f\"üìÅ Checkpoints directory: {CHECKPOINTS_DIR}\")\n",
    "print(f\"üè• Available modalities: {[info['name'] for info in MODALITIES.values()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a0048",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conditional_model(checkpoint_path=None, model_type=\"disentangled\"):\n",
    "    \"\"\"Load a conditional VAE model with optional checkpoint weights.\"\"\"\n",
    "    \n",
    "    if model_type == \"disentangled\":\n",
    "        model = DisentangledConditionalVAE(**DISENTANGLED_CONFIG)\n",
    "        config_used = DISENTANGLED_CONFIG\n",
    "    elif model_type == \"conditional\":\n",
    "        model = ConditionalVAE(**CONDITIONAL_CONFIG)\n",
    "        config_used = CONDITIONAL_CONFIG\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Choose 'disentangled' or 'conditional'\")\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"üìÇ Loading checkpoint from: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Extract model weights from Lightning checkpoint\n",
    "        model_state_dict = {}\n",
    "        for key, value in checkpoint[\"state_dict\"].items():\n",
    "            if key.startswith(\"model.\"):\n",
    "                model_state_dict[key[6:]] = value  # Remove \"model.\" prefix\n",
    "        \n",
    "        try:\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            print(\"‚úÖ Model weights loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading weights: {e}\")\n",
    "            print(\"Using randomly initialized weights...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No checkpoint provided - using randomly initialized weights\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"üß† Model loaded: {model.__class__.__name__}\")\n",
    "    print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    if model_type == \"disentangled\":\n",
    "        print(f\"üîÄ Shared latent dim: {config_used['shared_latent_dim']}\")\n",
    "        print(f\"üéØ Modality latent dim: {config_used['modality_latent_dim']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model selection widget\n",
    "model_type_widget = widgets.Dropdown(\n",
    "    options=[('Disentangled VAE', 'disentangled'), ('Conditional VAE', 'conditional')],\n",
    "    value='disentangled',\n",
    "    description='Model Type:'\n",
    ")\n",
    "\n",
    "checkpoint_path_widget = widgets.Text(\n",
    "    value='',  # Update with your checkpoint path\n",
    "    placeholder='Path to checkpoint file (.ckpt)',\n",
    "    description='Checkpoint:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(description=\"Load Model\", button_style='primary')\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_load_clicked(b):\n",
    "    with output_widget:\n",
    "        clear_output()\n",
    "        global model, model_type\n",
    "        model_type = model_type_widget.value\n",
    "        checkpoint = checkpoint_path_widget.value if checkpoint_path_widget.value.strip() else None\n",
    "        model = load_conditional_model(checkpoint, model_type)\n",
    "\n",
    "load_button.on_click(on_load_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([model_type_widget, load_button]),\n",
    "    checkpoint_path_widget,\n",
    "    output_widget\n",
    "]))\n",
    "\n",
    "# Initialize with default model\n",
    "model_type = 'disentangled'\n",
    "model = load_conditional_model(None, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ff7c2",
   "metadata": {},
   "source": [
    "## 4. Load Multi-Modal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b15d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.multimodal_dataset import MultiModalDataset\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Available datasets\n",
    "DATASETS = {\n",
    "    'chestmnist': 'data/chestmnist.npz',\n",
    "    'pathmnist': 'data/pathmnist.npz',\n",
    "    'octmnist': 'data/octmnist.npz',\n",
    "    'pneumoniamnist': 'data/pneumoniamnist.npz',\n",
    "    'dermamnist': 'data/dermamnist.npz'\n",
    "}\n",
    "\n",
    "# Data loading widgets\n",
    "dataset_selection = widgets.SelectMultiple(\n",
    "    options=list(DATASETS.keys()),\n",
    "    value=['chestmnist', 'pathmnist'],  # Default selection\n",
    "    description='Datasets:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px', height='120px')\n",
    ")\n",
    "\n",
    "batch_size_widget = widgets.IntSlider(\n",
    "    value=32,\n",
    "    min=8,\n",
    "    max=128,\n",
    "    step=8,\n",
    "    description='Batch Size:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "num_samples_widget = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=50,\n",
    "    max=500,\n",
    "    step=50,\n",
    "    description='Samples per Dataset:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_data_button = widgets.Button(\n",
    "    description='Load Selected Datasets',\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "# Output area\n",
    "data_output = widgets.Output()\n",
    "\n",
    "def load_multimodal_data(b=None):\n",
    "    with data_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            selected_datasets = list(dataset_selection.value)\n",
    "            if not selected_datasets:\n",
    "                print(\"Please select at least one dataset.\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Loading datasets: {selected_datasets}\")\n",
    "            \n",
    "            # Load all selected datasets\n",
    "            all_data = {}\n",
    "            for dataset_name in selected_datasets:\n",
    "                print(f\"Loading {dataset_name}...\")\n",
    "                data_path = DATASETS[dataset_name]\n",
    "                data = np.load(data_path)\n",
    "                \n",
    "                # Extract train/test data\n",
    "                train_images = data['train_images']\n",
    "                train_labels = data['train_labels']\n",
    "                test_images = data['test_images']\n",
    "                test_labels = data['test_labels']\n",
    "                \n",
    "                all_data[dataset_name] = {\n",
    "                    'train_images': train_images,\n",
    "                    'train_labels': train_labels,\n",
    "                    'test_images': test_images,\n",
    "                    'test_labels': test_labels,\n",
    "                    'num_classes': len(np.unique(train_labels))\n",
    "                }\n",
    "                \n",
    "                print(f\"  - Train: {train_images.shape}, Test: {test_images.shape}\")\n",
    "                print(f\"  - Classes: {len(np.unique(train_labels))}\")\n",
    "            \n",
    "            # Create combined dataset\n",
    "            global multimodal_dataset, dataloader, dataset_info\n",
    "            multimodal_dataset = MultiModalDataset(all_data, num_samples=num_samples_widget.value)\n",
    "            dataloader = DataLoader(multimodal_dataset, batch_size=batch_size_widget.value, shuffle=True)\n",
    "            \n",
    "            dataset_info = {\n",
    "                'modalities': selected_datasets,\n",
    "                'modality_to_idx': {mod: idx for idx, mod in enumerate(selected_datasets)},\n",
    "                'total_samples': len(multimodal_dataset),\n",
    "                'batch_size': batch_size_widget.value\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n‚úÖ Successfully loaded multi-modal dataset!\")\n",
    "            print(f\"Total samples: {len(multimodal_dataset)}\")\n",
    "            print(f\"Modalities: {selected_datasets}\")\n",
    "            print(f\"Batch size: {batch_size_widget.value}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "load_data_button.on_click(load_multimodal_data)\n",
    "\n",
    "# Display widgets\n",
    "print(\"Select datasets and configure data loading:\")\n",
    "display(widgets.VBox([\n",
    "    dataset_selection,\n",
    "    batch_size_widget,\n",
    "    num_samples_widget,\n",
    "    load_data_button,\n",
    "    data_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176bad6",
   "metadata": {},
   "source": [
    "## 5. Interactive Multi-Modal Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98daf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def reconstruct_samples():\n",
    "    \"\"\"Interactive reconstruction interface\"\"\"\n",
    "    \n",
    "    # Sample selection widgets\n",
    "    modality_dropdown = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Modality:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    sample_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=99,\n",
    "        description='Sample Index:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    reconstruct_button = widgets.Button(\n",
    "        description='Reconstruct Sample',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    reconstruction_output = widgets.Output()\n",
    "    \n",
    "    def update_modality_options():\n",
    "        if 'dataset_info' in globals():\n",
    "            modality_dropdown.options = dataset_info['modalities']\n",
    "            if dataset_info['modalities']:\n",
    "                modality_dropdown.value = dataset_info['modalities'][0]\n",
    "    \n",
    "    def perform_reconstruction(b=None):\n",
    "        with reconstruction_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if 'conditional_model' not in globals():\n",
    "                print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            if 'multimodal_dataset' not in globals():\n",
    "                print(\"‚ùå Please load dataset first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                # Get selected sample\n",
    "                modality_name = modality_dropdown.value\n",
    "                sample_idx = sample_slider.value\n",
    "                modality_idx = dataset_info['modality_to_idx'][modality_name]\n",
    "                \n",
    "                # Get sample from dataset\n",
    "                sample_data = multimodal_dataset[sample_idx]\n",
    "                image = sample_data['image'].unsqueeze(0).to(device)  # Add batch dimension\n",
    "                modality_condition = sample_data['modality'].unsqueeze(0).to(device)\n",
    "                class_condition = sample_data['class'].unsqueeze(0).to(device)\n",
    "                \n",
    "                # Perform reconstruction\n",
    "                conditional_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(conditional_model, 'encode'):\n",
    "                        # Get latent representation\n",
    "                        mu, log_var = conditional_model.encode(image, modality_condition, class_condition)\n",
    "                        z = conditional_model.reparameterize(mu, log_var)\n",
    "                        \n",
    "                        # Reconstruct\n",
    "                        reconstruction = conditional_model.decode(z, modality_condition, class_condition)\n",
    "                    else:\n",
    "                        # Use forward pass\n",
    "                        outputs = conditional_model(image, modality_condition, class_condition)\n",
    "                        reconstruction = outputs['reconstruction']\n",
    "                \n",
    "                # Convert to numpy for visualization\n",
    "                original_img = image.cpu().squeeze().numpy()\n",
    "                reconstructed_img = reconstruction.cpu().squeeze().numpy()\n",
    "                \n",
    "                # Handle different channel configurations\n",
    "                if original_img.ndim == 3 and original_img.shape[0] in [1, 3]:\n",
    "                    original_img = np.transpose(original_img, (1, 2, 0))\n",
    "                    reconstructed_img = np.transpose(reconstructed_img, (1, 2, 0))\n",
    "                \n",
    "                if original_img.shape[-1] == 1:\n",
    "                    original_img = original_img.squeeze(-1)\n",
    "                    reconstructed_img = reconstructed_img.squeeze(-1)\n",
    "                \n",
    "                # Create visualization\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                \n",
    "                # Original image\n",
    "                axes[0].imshow(original_img, cmap='gray' if len(original_img.shape) == 2 else None)\n",
    "                axes[0].set_title(f'Original\\\\n{modality_name} - Class {class_condition.item()}')\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                # Reconstructed image\n",
    "                axes[1].imshow(reconstructed_img, cmap='gray' if len(reconstructed_img.shape) == 2 else None)\n",
    "                axes[1].set_title(f'Reconstructed\\\\n{modality_name} - Class {class_condition.item()}')\n",
    "                axes[1].axis('off')\n",
    "                \n",
    "                # Difference\n",
    "                diff = np.abs(original_img - reconstructed_img)\n",
    "                im = axes[2].imshow(diff, cmap='hot')\n",
    "                axes[2].set_title('Absolute Difference')\n",
    "                axes[2].axis('off')\n",
    "                plt.colorbar(im, ax=axes[2])\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mse = F.mse_loss(reconstruction, image).item()\n",
    "                print(f\"\\\\nüìä Reconstruction Metrics:\")\n",
    "                print(f\"MSE Loss: {mse:.6f}\")\n",
    "                print(f\"Modality: {modality_name} (index: {modality_idx})\")\n",
    "                print(f\"Class: {class_condition.item()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during reconstruction: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    reconstruct_button.on_click(perform_reconstruction)\n",
    "    \n",
    "    # Update options when called\n",
    "    update_modality_options()\n",
    "    \n",
    "    # Display interface\n",
    "    print(\"Select a sample to reconstruct:\")\n",
    "    display(widgets.VBox([\n",
    "        modality_dropdown,\n",
    "        sample_slider,\n",
    "        reconstruct_button,\n",
    "        reconstruction_output\n",
    "    ]))\n",
    "\n",
    "# Call the function to create the interface\n",
    "reconstruct_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369701f",
   "metadata": {},
   "source": [
    "## 6. Conditional Generation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_generation_interface():\n",
    "    \"\"\"Interactive conditional generation interface\"\"\"\n",
    "    \n",
    "    # Generation parameter widgets\n",
    "    gen_modality_dropdown = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Target Modality:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    gen_class_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=1,\n",
    "        description='Target Class:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    num_samples_gen = widgets.IntSlider(\n",
    "        value=8,\n",
    "        min=1,\n",
    "        max=16,\n",
    "        description='Number of Samples:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    latent_dim_slider = widgets.IntSlider(\n",
    "        value=128,\n",
    "        min=64,\n",
    "        max=512,\n",
    "        step=64,\n",
    "        description='Latent Dimension:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    temperature_slider = widgets.FloatSlider(\n",
    "        value=1.0,\n",
    "        min=0.1,\n",
    "        max=2.0,\n",
    "        step=0.1,\n",
    "        description='Temperature:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    generate_button = widgets.Button(\n",
    "        description='Generate Samples',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    generation_output = widgets.Output()\n",
    "    \n",
    "    def update_generation_options():\n",
    "        if 'dataset_info' in globals():\n",
    "            gen_modality_dropdown.options = dataset_info['modalities']\n",
    "            if dataset_info['modalities']:\n",
    "                gen_modality_dropdown.value = dataset_info['modalities'][0]\n",
    "                \n",
    "                # Update class range based on dataset\n",
    "                modality_name = gen_modality_dropdown.value\n",
    "                if 'multimodal_dataset' in globals():\n",
    "                    # Get max class from the dataset\n",
    "                    max_class = 0\n",
    "                    for sample_idx in range(min(100, len(multimodal_dataset))):\n",
    "                        sample = multimodal_dataset[sample_idx]\n",
    "                        max_class = max(max_class, sample['class'].item())\n",
    "                    gen_class_slider.max = max_class\n",
    "    \n",
    "    def generate_conditional_samples(b=None):\n",
    "        with generation_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if 'conditional_model' not in globals():\n",
    "                print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                modality_name = gen_modality_dropdown.value\n",
    "                target_class = gen_class_slider.value\n",
    "                n_samples = num_samples_gen.value\n",
    "                latent_size = latent_dim_slider.value\n",
    "                temperature = temperature_slider.value\n",
    "                \n",
    "                modality_idx = dataset_info['modality_to_idx'][modality_name]\n",
    "                \n",
    "                print(f\"üé® Generating {n_samples} samples...\")\n",
    "                print(f\"Modality: {modality_name} (index: {modality_idx})\")\n",
    "                print(f\"Class: {target_class}\")\n",
    "                print(f\"Temperature: {temperature}\")\n",
    "                \n",
    "                conditional_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Sample from prior\n",
    "                    z = torch.randn(n_samples, latent_size).to(device) * temperature\n",
    "                    \n",
    "                    # Create condition tensors\n",
    "                    modality_condition = torch.full((n_samples,), modality_idx).to(device)\n",
    "                    class_condition = torch.full((n_samples,), target_class).to(device)\n",
    "                    \n",
    "                    # Generate samples\n",
    "                    if hasattr(conditional_model, 'decode'):\n",
    "                        generated_samples = conditional_model.decode(z, modality_condition, class_condition)\n",
    "                    else:\n",
    "                        # Use the model's generate method if available\n",
    "                        generated_samples = conditional_model.generate(z, modality_condition, class_condition)\n",
    "                    \n",
    "                    # Convert to numpy\n",
    "                    samples = generated_samples.cpu().numpy()\n",
    "                    \n",
    "                    # Create visualization grid\n",
    "                    cols = min(4, n_samples)\n",
    "                    rows = (n_samples + cols - 1) // cols\n",
    "                    \n",
    "                    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "                    if rows == 1:\n",
    "                        axes = axes.reshape(1, -1)\n",
    "                    if cols == 1:\n",
    "                        axes = axes.reshape(-1, 1)\n",
    "                    \n",
    "                    for i in range(n_samples):\n",
    "                        row = i // cols\n",
    "                        col = i % cols\n",
    "                        \n",
    "                        # Handle different channel configurations\n",
    "                        img = samples[i]\n",
    "                        if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                            img = np.transpose(img, (1, 2, 0))\n",
    "                        if img.shape[-1] == 1:\n",
    "                            img = img.squeeze(-1)\n",
    "                        \n",
    "                        axes[row, col].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                        axes[row, col].set_title(f'{modality_name}\\\\nClass {target_class}')\n",
    "                        axes[row, col].axis('off')\n",
    "                    \n",
    "                    # Hide empty subplots\n",
    "                    for i in range(n_samples, rows * cols):\n",
    "                        row = i // cols\n",
    "                        col = i % cols\n",
    "                        axes[row, col].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(f\"\\\\n‚úÖ Generated {n_samples} samples successfully!\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during generation: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    generate_button.on_click(generate_conditional_samples)\n",
    "    \n",
    "    # Update options when called\n",
    "    update_generation_options()\n",
    "    \n",
    "    # Display interface\n",
    "    print(\"Configure conditional generation parameters:\")\n",
    "    display(widgets.VBox([\n",
    "        gen_modality_dropdown,\n",
    "        gen_class_slider,\n",
    "        num_samples_gen,\n",
    "        latent_dim_slider,\n",
    "        temperature_slider,\n",
    "        generate_button,\n",
    "        generation_output\n",
    "    ]))\n",
    "\n",
    "# Call the function to create the interface\n",
    "conditional_generation_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117191a",
   "metadata": {},
   "source": [
    "## 7. Disentangled Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_exploration():\n",
    "    \"\"\"Explore disentangled latent space properties\"\"\"\n",
    "    \n",
    "    # Exploration widgets\n",
    "    exploration_type = widgets.Dropdown(\n",
    "        options=['Latent Interpolation', 'Dimension Analysis', 'Cross-Modal Transfer'],\n",
    "        value='Latent Interpolation',\n",
    "        description='Exploration Type:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    source_modality = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Source Modality:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    target_modality = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='Target Modality:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    source_class = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=1,\n",
    "        description='Source Class:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    target_class = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=1,\n",
    "        description='Target Class:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    num_steps = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=5,\n",
    "        max=20,\n",
    "        description='Interpolation Steps:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    latent_dim_to_vary = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=127,\n",
    "        description='Latent Dimension:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    variation_range = widgets.FloatSlider(\n",
    "        value=3.0,\n",
    "        min=1.0,\n",
    "        max=5.0,\n",
    "        step=0.5,\n",
    "        description='Variation Range:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    explore_button = widgets.Button(\n",
    "        description='Explore Latent Space',\n",
    "        button_style='warning'\n",
    "    )\n",
    "    \n",
    "    exploration_output = widgets.Output()\n",
    "    \n",
    "    def update_exploration_options():\n",
    "        if 'dataset_info' in globals():\n",
    "            modalities = dataset_info['modalities']\n",
    "            source_modality.options = modalities\n",
    "            target_modality.options = modalities\n",
    "            if modalities:\n",
    "                source_modality.value = modalities[0]\n",
    "                target_modality.value = modalities[-1] if len(modalities) > 1 else modalities[0]\n",
    "    \n",
    "    def perform_exploration(b=None):\n",
    "        with exploration_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if 'conditional_model' not in globals():\n",
    "                print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            if 'multimodal_dataset' not in globals():\n",
    "                print(\"‚ùå Please load dataset first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                exploration_mode = exploration_type.value\n",
    "                print(f\"üîç {exploration_mode}...\")\n",
    "                \n",
    "                conditional_model.eval()\n",
    "                \n",
    "                if exploration_mode == 'Latent Interpolation':\n",
    "                    # Interpolate between two samples\n",
    "                    source_mod_idx = dataset_info['modality_to_idx'][source_modality.value]\n",
    "                    target_mod_idx = dataset_info['modality_to_idx'][target_modality.value]\n",
    "                    \n",
    "                    # Get random samples from each modality/class\n",
    "                    with torch.no_grad():\n",
    "                        # Create condition tensors\n",
    "                        source_mod_tensor = torch.tensor([source_mod_idx]).to(device)\n",
    "                        source_class_tensor = torch.tensor([source_class.value]).to(device)\n",
    "                        target_mod_tensor = torch.tensor([target_mod_idx]).to(device)\n",
    "                        target_class_tensor = torch.tensor([target_class.value]).to(device)\n",
    "                        \n",
    "                        # Sample latent codes\n",
    "                        latent_size = latent_dim_slider.value if 'latent_dim_slider' in globals() else 128\n",
    "                        z1 = torch.randn(1, latent_size).to(device)\n",
    "                        z2 = torch.randn(1, latent_size).to(device)\n",
    "                        \n",
    "                        # Interpolation\n",
    "                        steps = num_steps.value\n",
    "                        interpolations = []\n",
    "                        \n",
    "                        for i in range(steps):\n",
    "                            alpha = i / (steps - 1)\n",
    "                            z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "                            \n",
    "                            # Generate with source modality first, then target\n",
    "                            if i < steps // 2:\n",
    "                                gen_sample = conditional_model.decode(z_interp, source_mod_tensor, source_class_tensor)\n",
    "                                mod_name = source_modality.value\n",
    "                            else:\n",
    "                                gen_sample = conditional_model.decode(z_interp, target_mod_tensor, target_class_tensor)\n",
    "                                mod_name = target_modality.value\n",
    "                            \n",
    "                            interpolations.append((gen_sample.cpu().numpy(), mod_name))\n",
    "                        \n",
    "                        # Visualize interpolation\n",
    "                        fig, axes = plt.subplots(2, steps//2, figsize=(steps*2, 6))\n",
    "                        \n",
    "                        for i, (sample, mod_name) in enumerate(interpolations):\n",
    "                            row = 0 if i < steps // 2 else 1\n",
    "                            col = i % (steps // 2)\n",
    "                            \n",
    "                            img = sample.squeeze()\n",
    "                            if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                                img = np.transpose(img, (1, 2, 0))\n",
    "                            if img.shape[-1] == 1:\n",
    "                                img = img.squeeze(-1)\n",
    "                            \n",
    "                            axes[row, col].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                            axes[row, col].set_title(f'{mod_name}\\\\nStep {i+1}')\n",
    "                            axes[row, col].axis('off')\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                \n",
    "                elif exploration_mode == 'Dimension Analysis':\n",
    "                    # Vary specific latent dimensions\n",
    "                    mod_idx = dataset_info['modality_to_idx'][source_modality.value]\n",
    "                    class_idx = source_class.value\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        base_z = torch.randn(1, latent_dim_slider.value if 'latent_dim_slider' in globals() else 128).to(device)\n",
    "                        mod_tensor = torch.tensor([mod_idx]).to(device)\n",
    "                        class_tensor = torch.tensor([class_idx]).to(device)\n",
    "                        \n",
    "                        # Vary the selected dimension\n",
    "                        dim_to_vary = latent_dim_to_vary.value\n",
    "                        var_range = variation_range.value\n",
    "                        variations = np.linspace(-var_range, var_range, num_steps.value)\n",
    "                        \n",
    "                        samples = []\n",
    "                        for var_val in variations:\n",
    "                            z_varied = base_z.clone()\n",
    "                            z_varied[0, dim_to_vary] = var_val\n",
    "                            \n",
    "                            sample = conditional_model.decode(z_varied, mod_tensor, class_tensor)\n",
    "                            samples.append(sample.cpu().numpy())\n",
    "                        \n",
    "                        # Visualize variations\n",
    "                        fig, axes = plt.subplots(1, len(samples), figsize=(len(samples)*2, 3))\n",
    "                        if len(samples) == 1:\n",
    "                            axes = [axes]\n",
    "                        \n",
    "                        for i, sample in enumerate(samples):\n",
    "                            img = sample.squeeze()\n",
    "                            if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                                img = np.transpose(img, (1, 2, 0))\n",
    "                            if img.shape[-1] == 1:\n",
    "                                img = img.squeeze(-1)\n",
    "                            \n",
    "                            axes[i].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                            axes[i].set_title(f'Dim {dim_to_vary}\\\\nVal: {variations[i]:.2f}')\n",
    "                            axes[i].axis('off')\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                \n",
    "                elif exploration_mode == 'Cross-Modal Transfer':\n",
    "                    # Transfer latent code between modalities\n",
    "                    source_mod_idx = dataset_info['modality_to_idx'][source_modality.value]\n",
    "                    target_mod_idx = dataset_info['modality_to_idx'][target_modality.value]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # Sample in source modality\n",
    "                        z = torch.randn(1, latent_dim_slider.value if 'latent_dim_slider' in globals() else 128).to(device)\n",
    "                        source_mod_tensor = torch.tensor([source_mod_idx]).to(device)\n",
    "                        target_mod_tensor = torch.tensor([target_mod_idx]).to(device)\n",
    "                        source_class_tensor = torch.tensor([source_class.value]).to(device)\n",
    "                        target_class_tensor = torch.tensor([target_class.value]).to(device)\n",
    "                        \n",
    "                        # Generate in both modalities\n",
    "                        source_sample = conditional_model.decode(z, source_mod_tensor, source_class_tensor)\n",
    "                        target_sample = conditional_model.decode(z, target_mod_tensor, target_class_tensor)\n",
    "                        \n",
    "                        # Visualize transfer\n",
    "                        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "                        \n",
    "                        for i, (sample, mod_name) in enumerate([(source_sample, source_modality.value), \n",
    "                                                              (target_sample, target_modality.value)]):\n",
    "                            img = sample.cpu().numpy().squeeze()\n",
    "                            if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                                img = np.transpose(img, (1, 2, 0))\n",
    "                            if img.shape[-1] == 1:\n",
    "                                img = img.squeeze(-1)\n",
    "                            \n",
    "                            axes[i].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                            axes[i].set_title(f'{mod_name}\\\\nSame Latent Code')\n",
    "                            axes[i].axis('off')\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                \n",
    "                print(f\"\\\\n‚úÖ {exploration_mode} completed!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during exploration: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    explore_button.on_click(perform_exploration)\n",
    "    \n",
    "    # Update options when called\n",
    "    update_exploration_options()\n",
    "    \n",
    "    # Create dynamic interface based on exploration type\n",
    "    def update_interface(*args):\n",
    "        mode = exploration_type.value\n",
    "        if mode == 'Latent Interpolation':\n",
    "            interface_widgets = [exploration_type, source_modality, target_modality, \n",
    "                               source_class, target_class, num_steps]\n",
    "        elif mode == 'Dimension Analysis':\n",
    "            interface_widgets = [exploration_type, source_modality, source_class, \n",
    "                               latent_dim_to_vary, variation_range, num_steps]\n",
    "        else:  # Cross-Modal Transfer\n",
    "            interface_widgets = [exploration_type, source_modality, target_modality, \n",
    "                               source_class, target_class]\n",
    "        \n",
    "        interface_widgets.extend([explore_button, exploration_output])\n",
    "        \n",
    "        # Clear and redisplay\n",
    "        exploration_output.clear_output()\n",
    "        with exploration_output:\n",
    "            print(\"Configure latent space exploration:\")\n",
    "            display(widgets.VBox(interface_widgets))\n",
    "    \n",
    "    exploration_type.observe(update_interface, names='value')\n",
    "    \n",
    "    # Initial display\n",
    "    print(\"Configure latent space exploration:\")\n",
    "    display(widgets.VBox([\n",
    "        exploration_type,\n",
    "        source_modality,\n",
    "        target_modality,\n",
    "        source_class,\n",
    "        target_class,\n",
    "        num_steps,\n",
    "        explore_button,\n",
    "        exploration_output\n",
    "    ]))\n",
    "\n",
    "# Call the function to create the interface\n",
    "latent_space_exploration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1988af",
   "metadata": {},
   "source": [
    "## 8. Model Analysis & Comparison\n",
    "\n",
    "This notebook provides comprehensive tools for exploring conditional and disentangled VAE models:\n",
    "\n",
    "### Features:\n",
    "- **Multi-Modal Data Loading**: Load and combine multiple MedMNIST datasets\n",
    "- **Interactive Reconstruction**: Compare original vs reconstructed images across modalities\n",
    "- **Conditional Generation**: Generate samples conditioned on modality and class\n",
    "- **Latent Space Exploration**: Interpolate, analyze dimensions, and transfer between modalities\n",
    "- **Disentanglement Analysis**: Explore how the latent space separates modalities and classes\n",
    "\n",
    "### Usage Tips:\n",
    "1. Start by loading a trained model using the model loading interface\n",
    "2. Load your desired datasets (recommend starting with 2-3 modalities)\n",
    "3. Use reconstruction to verify model quality\n",
    "4. Experiment with conditional generation across different modalities\n",
    "5. Explore latent space properties to understand disentanglement\n",
    "\n",
    "### Next Steps:\n",
    "- Compare with the vanilla VAE notebook for single-modality behavior\n",
    "- Experiment with different temperature settings for generation\n",
    "- Analyze cross-modal relationships in the latent space\n",
    "- Use dimension analysis to understand which latent dimensions control specific features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
