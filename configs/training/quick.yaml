# @package _global_

defaults:
    - default

training:
    max_epochs: 10 # Fewer epochs for quick training
    gradient_clip_val: 1.0
    accumulate_grad_batches: 1

    optimizer:
        type: adam # Faster than AdamW
        lr: 0.001 # Higher learning rate
        weight_decay: 0.0
        betas: [0.9, 0.999]

    scheduler:
        type: step # Simple step scheduler
        step_size: 5
        gamma: 0.5

    loss:
        type: vae # Vanilla VAE loss
        recon_loss_type: mse
        kl_weight: 1.0
        recon_weight: 1.0

    val_check_interval: 0.5 # Check validation twice per epoch
    check_val_every_n_epoch: 1
    log_every_n_steps: 20 # Log more frequently
    save_top_k: 1 # Save only best model
